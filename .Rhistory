# Load necessary libraries for Arrow file system, Spark interaction, and data manipulation
library(arrow)    # Handle efficient data exchange between R and Spark
library(sparklyr)   # Interface to Apache Spark
library(dplyr)      # Data manipulation and transformation
# Install Spark version 3.5.5 if not already installed
spark_install("3.5.5")
# Set environment variables for Java and Spark to enable Spark to run properly
Sys.setenv("JAVA_HOME" = "/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home")
Sys.setenv("SPARK_HOME" = spark_home_dir(version = "3.5.5"))
# Set working directory where project files and data are located
working_dir <- "/Users/rodgersiradukunda/Library/CloudStorage/OneDrive-TheUniversityofLiverpool/geospatial_docker/sedona-tutorial"
setwd(working_dir)
# Set directory where Spark temporary data will be stored
spark_dir <- file.path(getwd(), "data", "spark")
# Create an empty configuration list for Spark settings
config <- list()
# Set Delta Lake specific extensions to enable Delta features in Spark SQL
config$spark.sql.extensions <- "io.delta.sql.DeltaSparkSessionExtension"
config$spark.sql.catalog.spark_catalog <- "org.apache.spark.sql.delta.catalog.DeltaCatalog"
# Use KryoSerializer for faster serialization
config$spark.serializer <- "org.apache.spark.serializer.KryoSerializer"
# Set temporary directory for Spark to write intermediate files
config$`sparklyr.shell.driver-java-options` <- paste0("-Djava.io.tmpdir=", spark_dir)
# Use compressed object pointers for JVM optimization
config$`sparklyr.shell.driver-java-options` <- "-XX:+UseCompressedOops"
# Set driver memory allocation to 10GB to handle large datasets
config$`sparklyr.shell.driver-memory` <- '10G'
# Allocate 70% of heap memory for Spark operations
config$spark.memory.fraction <- 0.7
# Set number of shuffle partitions based on workload to optimize parallel processing
config$spark.sql.shuffle.partitions.local <- 24
# Set extra Java options for driver including heap space allocation
config$spark.driver.extraJavaOptions <- "-Xmx1G"
# Enable off-heap memory and allocate 2GB for off-heap storage
config$spark.memory.offHeap.enabled <- "true"
config$spark.memory.offHeap.size <- "2g"
# Disable shuffle spill to disk to reduce I/O overhead
config$spark.sql.shuffle.spill <- "false"
# Set periodic garbage collection to every 60 seconds
config$spark.cleaner.periodicGC.interval <- "60s"
# Set max partition size for shuffle files to 200MB
config$spark.sql.files.maxPartitionBytes <- "200m"
# Enable adaptive query execution for optimized performance
config$spark.sql.adaptive.enabled <- "true"
# Establish connection to Spark with specified configuration and load necessary packages
sc <- spark_connect(
master = "local[*]",  # Run Spark locally utilizing all cores
config = config,  # Apply defined configurations
packages = c(
"io.delta:delta-spark_2.12:3.3.0",   # Delta Lake for ACID transactions
"org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0",  # Sedona for spatial processing
"org.datasyslab:geotools-wrapper:1.7.0-28.5"  # GeoTools for spatial data support
)
# Load Apache Sedona library for geospatial analysis
library(apache.sedona)
# Initialize Sedona context to enable spatial functions
invoke_static(
sc,
"org.apache.sedona.spark.SedonaContext",
"create",
spark_session(sc),
"r"
)
# Read locations dataset in Delta format and register as a SQL view for querying
locations_sdf_updated_three <- spark_read_delta(
sc,
path = file.path(getwd(), "data", "locations_sdf_updated_three")
) |>
sdf_repartition(partitions = 24, partition_by = "trip_id") %>%  # Repartition data for efficiency
sdf_register("locations_sdf_updated_three_view")  # Register as view for SQL queries
# Read trip data in Delta format and register as a SQL view
trip_data_sdf <- spark_read_delta(
sc,
path = file.path(getwd(), "data", "trip_data_sdf")
) %>%
sdf_repartition(partitions = 24, partition_by = "trip_id") %>%  # Repartition for optimal join performance
sdf_register("trip_data_sdf")
trip_data_sdf
# Open Spark Web UI to monitor jobs and cluster resources
spark_web(sc)
# Join trip data with pickup locations using trip_id and rename selected columns for clarity
merged_one <- left_join(
trip_data_sdf,
locations_sdf_updated_three %>% filter(is_pickup == 1),
by = "trip_id"
) %>%
rename(
pickup_borough = BoroName,
pickup_neighbourhood = NTAName,
pickup_neigh_hhincome = MdHHIncE,
pickup_pop_density = pop_density,
pickup_lcz_label = lcz_label
) %>%
select(
trip_id,
VendorID,
tpep_pickup_datetime,
tpep_dropoff_datetime,
passenger_count,
trip_distance,
pickup_hour,
pickup_dayofweek,
pickup_week,
pickup_month,
dropoff_hour,
dropoff_dayofweek,
dropoff_week,
dropoff_month,
is_weekend_pickup,
is_weekend_dropoff,
is_rush_hour_pickup,
trip_distance_scaled,
pickup_borough,
pickup_neighbourhood,
pickup_neigh_hhincome,
pickup_pop_density,
pickup_lcz_label
)
# Join the merged pickup dataset with dropoff locations and rename/drop unnecessary columns
merged_two <- left_join(
merged_one,
locations_sdf_updated_three %>% filter(is_pickup == 0),
by = "trip_id"
) %>%
rename(
dropoff_borough = BoroName,
dropoff_neighbourhood = NTAName,
dropoff_neigh_hhincome = MdHHIncE,
dropoff_pop_density = pop_density,
dropoff_lcz_label = lcz_label
) %>%
select(-c(latitude, longitude, is_pickup, lcz_class))  # Drop redundant columns
# Define output path to save final merged dataset in Delta format
locations_sdf_updated_four_file_path <- file.path(
getwd(),
"data",
"locations_sdf_updated_four2"
)
# Write final dataset in Delta format with append mode to allow incremental writing
spark_write_delta(
merged_two,
path = locations_sdf_updated_four_file_path,
mode = "append"
)
# Disconnect from the Spark session to free resources
spark_disconnect(sc)
