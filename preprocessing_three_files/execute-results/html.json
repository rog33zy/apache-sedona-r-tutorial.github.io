{
  "hash": "f6f2ab831093da963733f15abdd72593",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 3: Part One - Processing Raster Data with Apache Sedona and Sparklyr in R\"\nsubtitle: \"Using Raster Data to Determine Population Density Around Pickup and Dropoff Points\"\nexecute: \n  eval: false\n  output: true\nformat: \n  html:\n    self-contained: true\n    toc: true\n    df-print: kable\neditor: visual\n---\n\n\n## Introduction\n\nWe are going to demonstrate how to obtain information from **raster files** based on coordinates in this chapter. Let us assume that there is a relationship between the **duration of a taxi ride** and the **population density** of an area. We will, therefore, need to extract population density values at each **pickup and dropoff location**. Such granular data is typically available in **raster format**, which is why we use **WorldPop population density data** with a resolution of **1 km by 1 km** for this purpose. You can download the data [here](https://www.worldpop.org) to follow along.\n\nThe **Spark configuration** used for this chapter — and the next one — is **identical** to that used in **Chapter 2**, so it has **not been included below for brevity's sake**.\n\nFurthermore, because I need to **re-run this code multiple times** to render this website, I shall **filter only a few rows** from the **94 million** we previously worked with, as I will be constantly updating this site. **In the actual analysis I conducted, however, I used the full dataset.**\n\nYou can find out more about using **Apache Sedona for raster manipulation** [here](https://sedona.apache.org).\n\n\n::: {.cell}\n\n:::\n\n\n## Loading updated locations data\n\nWe start by **loading our updated locations data**, which now contains **household median income by neighbourhood** information.\n\nTo reiterate, I will **filter for about 14 million rows** here so that I can **render this webpage faster**. However, sometimes in your analysis, you will find that you have **too much data and limited memory**, especially when running **complex transformations**.\n\nIn such cases, you can **filter for specific rows**, perform your analysis on that subset, and then **append the results to your delta tables**. You can repeat this process for another set of rows until you are done.\n\nFor instance, knowing that I have **trip ID values ranging from 0 to about 48,000,000**, I would:\n\n-   First filter for rows between **0 and 16 million**,\\\n-   Then **16 million to 32 million**,\\\n-   And finally, anything **above 32 million**,\\\n-   **Appending to the same folder** each time.\n\nIf you have **enough RAM and cores**, though, **feel free to run everything at once** — go crazy with it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define path to the updated locations Delta table\nlocations_sdf_updated_one <- spark_read_delta(\n  sc,\n  path = file.path(\n    getwd(), \n    \"data\", \n    \"locations_sdf_updated_one\"\n  )\n) |> \n  filter(trip_id > 40000000) %>% \n  sdf_repartition(partitions = 24) %>%  # Repartition for better parallel processing\n  sdf_register(\"locations_sdf_updated_one_view\")  # Register as a temporary view\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the size of each partition in the dataframe\nlocations_sdf_updated_one %>% sdf_partition_sizes()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the number of rows in the dataframe\nlocations_sdf_updated_one |> \n  sdf_nrow()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(locations_sdf_updated_one, width=Inf)\n```\n:::\n\n\n## Loading WorldPop Population Density dataset\n\nThe difference when using **raster data** compared to vector data with **Apache Sedona** is that we **do not import raster in its native format directly**. Instead, we must **first load it as a binary dataframe**, and **then convert it into its native raster format** within Sedona.\n\nAlso, **bear in mind** that **Sedona only accepts raster files** in the following formats:\\\n- **Arc Info ASCII Grid**,\\\n- **GeoTIFF**, and\\\n- **NetCDF**.\n\nIf your data is in **any other raster format**, you will first need to **convert it to one of these supported formats**.\n\nI have found **GDAL** to be **particularly useful for converting between different raster formats** — definitely a tool to keep in your geospatial toolbox!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the raster data for world population (NYC)\nworld_pop_raster_filepath <- file.path(\n  getwd(),\n  \"data\",\n  \"raster\",\n  \"worldpop\",\n  \"nyc.tif\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the raster data as a binary file\nworld_pop_binary <- spark_read_binary(\n  sc,\n  dir = world_pop_raster_filepath,\n  name = \"worldpop\"\n)\n```\n:::\n\n\nWe obtain raster geometry from our GeoTiff data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Register the world population raster as a temporary view\nworld_pop_binary |> sdf_register(\"worldpop_view\")\n\n# Extract raster data from the GeoTiff file using Sedona\nworldpop_sdf <- sdf_sql(\n  sc,\n  \"\n  SELECT RS_FromGeoTiff(content) AS raster FROM worldpop_view\n  \"\n)\n\n# Register the raster data as a temporary view\nworldpop_sdf |> sdf_register(\"worldpop_view\") |> compute()\n\nworldpop_sdf %>% glimpse()\n```\n:::\n\n\nWe can **retrieve metadata** from our raster file, including:\\\n- The **upper left coordinates** of the raster (in the raster’s coordinate system units),\\\n- The **width and height** of the raster (in number of pixels),\\\n- The **spatial resolution** of each pixel (in units of the raster’s CRS),\\\n- Any **skew or rotation** of the raster (if present),\\\n- The **SRID** (spatial reference system identifier) of the raster’s coordinate system,\\\n- The **number of bands**, and\\\n- **Tile width and height**.\n\nIn our case:\\\n- **Upper left X coordinate**: `-74.25125`\\\n- **Upper left Y coordinate**: `40.90792` (both in degrees as the CRS is WGS84)\\\n- **Raster size**: `66 x 49` pixels (quite small)\\\n- **Pixel resolution**: `0.00833 x -0.00833` degrees\\\n- **Skew**: `0` in both x and y directions (i.e., no skew)\\\n- **SRID**: `4326` (WGS 84)\\\n- **Number of bands**: `2`\\\n- **Tile width**: `66`, **Tile height**: `15`\n\nAll this information is important when **interpreting and working with raster data**, especially when performing **coordinate-based queries**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Retrieve and view metadata for the world population raster\nworldpop_sdf_metadata <- sdf_sql(\n  sc,\n  \"\n  SELECT RS_MetaData(raster) FROM worldpop_view\n  \"\n) |> collect()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Glimpse at the metadata information\nworldpop_sdf_metadata |> glimpse()\n```\n:::\n\n\n## Joining point data with raster data\n\nWe now **conduct the join using Spatial SQL**, as it is much easier and more intuitive than using Apache Sedona's R functions for raster operations.\n\nBy leveraging **Spatial SQL**, we can directly query raster values at specific pickup and dropoff coordinates, simplifying what would otherwise be a more complex process if done via function-based syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform a spatial join between the locations and the world population data to calculate population density\nlocations_sdf_updated_two <- sdf_sql(\n  sc,\n  \"\n  SELECT \n    /*+ BROADCAST(w) */ l.*, RS_Value(w.raster, ST_Point(l.longitude, l.latitude)) AS pop_density\n  FROM\n    locations_sdf_updated_one_view l\n  LEFT JOIN worldpop_view w\n    ON RS_Intersects(w.raster, ST_POINT(l.longitude, l.latitude))\n  \"\n) \n```\n:::\n\n\nWe can now take a look at the result of our join below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Glimpse at the updated data with population density\nlocations_sdf_updated_two %>% glimpse()\n\n# Print a preview of the resulting dataframe with specific formatting options\nwithr::with_options(\n  list(pillar.sigfig = 11),\n  print(locations_sdf_updated_two, n=30)\n)\n```\n:::\n\n\n## Saving the data\n\nAnd finally save the data to file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define file path for saving the updated dataframe\nlocations_sdf_updated_two_file_path <- file.path(\n  getwd(), \n  \"data\", \n  \"locations_sdf_updated_two\"\n)\n\n# Save the final dataframe as a Delta table\nspark_write_delta(\n  locations_sdf_updated_two,\n  path = locations_sdf_updated_two_file_path,\n  mode = \"append\"  # Overwrite any existing data at the location\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Disconnect from the Spark session\nspark_disconnect(sc)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}