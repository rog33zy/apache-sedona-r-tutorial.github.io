{
  "hash": "59bf8b1d41517a109e0c89d9eb21708b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R\"\nsubtitle: \"Using Raster Tiles to Determine Local Climate Zones of Pickup and Dropoff Locations\"\nexecute: \n  eval: true\n  output: true\n---\n\n\n## Introduction\n\nFor this chapter, we would like to find out the land cover classification associated with specific pickup and dropoff points. Our assumption is that the type of area where one picks up a taxi or gets dropped off may influence how long the trip takes. Again, do not dwell too much on this assumption — the main objective is to demonstrate another way of extracting data from raster files.\n\nWe shall make use of the **Local Climate Zones (LCZ) Map** from the **World Urban Database and Access Portal Tools (WUDAPT)**. The US version of this dataset can be accessed [here](https://figshare.com/articles/dataset/CONUS-wide_LCZ_map_and_Training_Areas/11416950). This dataset contains **17 urban land cover classifications**, ranging from compact high-rise buildings to water bodies.\n\nWe downloaded a version that was already in **EPSG:4326 CRS** and clipped it based on the NYC boundary. For this chapter, we demonstrate how to divide a raster image into tiles before performing analysis. This is particularly useful when working with large raster files that cannot be processed in their entirety. By dividing the raster into tiles, we make the analysis more manageable and efficient.\n\n**Note**: Raster processing is computationally intensive, so it is preferable to work with smaller files, especially when operating in local mode. You will know a raster file is too large when you receive an error upon attempting to view its contents.\n\n**Important**: The Spark configuration used in this chapter is the same as that used in Chapter 2, and so it is not repeated here for brevity.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'arrow'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:utils':\n\n    timestamp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'sparklyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<jobj[23]>\n  org.apache.spark.sql.SparkSession\n  org.apache.spark.sql.SparkSession@623617\n```\n\n\n:::\n:::\n\n\n## Loading the data\n\nWe start by loading our most updated locations data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read location data stored in Delta format\nlocations_sdf_updated_two <- spark_read_delta(\n  sc,\n  path = file.path(getwd(), \"data\", \"locations_sdf_updated_two\")\n) |> \n  filter(trip_id > 40000000) %>%  # Optional filtering\n  sdf_repartition(partitions = 24) %>%  # Repartition for parallelism\n  sdf_register(\"locations_sdf_updated_two_view\")  # Register as temporary view for SQL\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check partition sizes to understand data distribution\nlocations_sdf_updated_two %>% \n  sdf_partition_sizes()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   partition_index partition_size\n1                0         578737\n2                1         578736\n3                2         578737\n4                3         578738\n5                4         578738\n6                5         578738\n7                6         578737\n8                7         578737\n9                8         578737\n10               9         578735\n11              10         578735\n12              11         578735\n13              12         578735\n14              13         578734\n15              14         578734\n16              15         578734\n17              16         578735\n18              17         578736\n19              18         578736\n20              19         578735\n21              20         578735\n22              21         578736\n23              22         578736\n24              23         578736\n```\n\n\n:::\n:::\n\n\nWe then load our raster data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define path to raster file (LCZ map)\nwudapt_raster_filepath <- file.path(\n  getwd(),\n  \"data\",\n  \"raster\",\n  \"wudapt\",\n  \"CONUS_LCZ_map_NLCD_v1.0_cropped.tif\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read raster as binary using Spark\nwudapt_binary <- spark_read_binary(\n  sc,\n  dir = wudapt_raster_filepath,\n  name = \"wudapt_binary_view\"\n)\n```\n:::\n\n\n## Creating raster tiles\n\nHere, we are **assuming that our raster data is too large to be processed in its entirety**. To address this, we **divide it into 256 by 256 tiles**. By doing so, we make it more analysis-friendly, as we can first identify a specific tile to work on and then perform analysis on only that tile, instead of the entire raster.\n\n**Note**: The clipped dataset used here was actually not large, but we proceeded with this method for demonstration purposes to show how one would handle genuinely large raster datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explode raster into tiles (256x256) for spatial operations and cache in memory\nwudapt_raster_tiles <- sdf_sql(\n  sc,\n  \"\n  SELECT RS_TileExplode(RS_FromGeoTiff(content), 256, 256) AS (x, y, tile) FROM wudapt_binary_view\n  \"\n) %>%\n  sdf_register(\"wudapt_raster_tiles_view\")\n\n# Quick look at raster tiles structure\nwudapt_raster_tiles %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: ??\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumns: 3\nDatabase: spark_connection\n$ x    <int> 0, 1, 0, 1\n$ y    <int> 0, 0, 1, 1\n$ tile <arrw_bnr> <00, 0f, 00, 00, 00, 67, 65, 6e, 65, 72, 69, 63, 43, 6f, 76, 65, …\n```\n\n\n:::\n:::\n\n\nAs you can see below, the spatial SQL query is divided into two parts:\n\n1.  First, we locate the tile that a given coordinate pair (pickup or dropoff point) belongs to.\\\n2.  Second, we use that specific tile to find the actual land classification value associated with that location.\n\nThis two-step approach makes it efficient to work with large raster datasets by limiting the analysis to only relevant tiles, rather than processing the entire raster at once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Spatial join: match location points with corresponding LCZ tiles and assign LCZ labels\nlocations_sdf_updated_three <- sdf_sql(\n  sc,\n  \"\n  WITH matched_tiles AS (\n    SELECT l.*, w.tile\n      FROM wudapt_raster_tiles_view w\n    JOIN locations_sdf_updated_two_view l\n      ON RS_Intersects(w.tile, ST_Point(l.longitude, l.latitude))\n  )\n\n  SELECT *,\n    TRY_CAST(RS_Value(tile, ST_Point(longitude, latitude)) AS INT) AS lcz_class,\n    CASE\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 1 THEN 'Compact highrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 2 THEN 'Compact midrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 3 THEN 'Compact lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 4 THEN 'Open highrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 5 THEN 'Open midrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 6 THEN 'Open lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 7 THEN 'Lightweight lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 8 THEN 'Large lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 9 THEN 'Sparsely built'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 10 THEN 'Heavy industry'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 11 THEN 'Dense trees'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 12 THEN 'Scattered trees'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 13 THEN 'Bush, scrub'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 14 THEN 'Low plants'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 15 THEN 'Bare rock or paved'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 16 THEN 'Bare soil or sand'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 17 THEN 'Water'\n      ELSE 'Unknown'\n    END AS lcz_label\n  FROM matched_tiles\n  \"\n)\n```\n:::\n\n\nWe now **remove the tile geometry** from our dataset because delta cannot serialise geometry columns, and also because we do not need the geometry for our intended analysis. This helps keep the dataset clean and light for storage and further processing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop RasterUDT (tile) column before saving as Delta (unsupported type in Delta)\nlocations_sdf_updated_three <- locations_sdf_updated_three %>%\n  select(-tile)\n```\n:::\n\n\nThe updated data now looks as shown below. As you can see, we went from only having coordinates to obtaining richer information about our locations by leveraging additional geospatial datasets.\n\nNot too bad for using a simple laptop to process tens of millions of rows!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview updated data\nlocations_sdf_updated_three %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: ??\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nColumns: 11\nDatabase: spark_connection\n$ trip_id     <dbl> 40935559, 43021415, 41275918, 40003363, 46280147, 43409493…\n$ latitude    <dbl> 40.74024, 40.74848, 40.73320, 40.64342, 40.72959, 40.77692…\n$ longitude   <dbl> -73.99476, -73.97615, -73.98975, -73.78960, -73.99666, -73…\n$ is_pickup   <dbl> 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1…\n$ BoroName    <chr> \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Queens\", \"Manhatta…\n$ NTA2020     <chr> \"MN0401\", \"MN0603\", \"MN0303\", \"QN8381\", \"MN0202\", \"MN0701\"…\n$ NTAName     <chr> \"Chelsea-Hudson Yards\", \"Murray Hill-Kips Bay\", \"East Vill…\n$ MdHHIncE    <int> 118915, 138337, 72063, NA, 175436, 158165, NA, 138337, 133…\n$ pop_density <dbl> 37463.500, 29922.352, 29772.838, 0.000, 31733.820, 23416.4…\n$ lcz_class   <int> 2, 1, 2, 15, 1, 1, 12, 1, 2, 2, 2, 1, 1, 1, 4, 1, 5, 15, 1…\n$ lcz_label   <chr> \"Compact midrise\", \"Compact highrise\", \"Compact midrise\", …\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print formatted preview for review\nwithr::with_options(\n  list(pillar.sigfig = 8),\n  print(locations_sdf_updated_three, n = 10, width = Inf)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   SQL [?? x 11]\n# Database: spark_connection\n    trip_id  latitude  longitude is_pickup BoroName  NTA2020\n      <dbl>     <dbl>      <dbl>     <dbl> <chr>     <chr>  \n 1 40935559 40.740238 -73.994759         1 Manhattan MN0401 \n 2 43021415 40.748482 -73.976151         1 Manhattan MN0603 \n 3 41275918 40.733196 -73.989754         1 Manhattan MN0303 \n 4 40003363 40.643421 -73.789597         1 Queens    QN8381 \n 5 46280147 40.729591 -73.996658         0 Manhattan MN0202 \n 6 43409493 40.776917 -73.985748         0 Manhattan MN0701 \n 7 41405153 40.766724 -73.978157         1 Manhattan MN6491 \n 8 44295520 40.745274 -73.980881         0 Manhattan MN0603 \n 9 43721917 40.771179 -73.949768         1 Manhattan MN0803 \n10 41236576 40.795189 -73.933403         1 Manhattan MN1102 \n   NTAName                               MdHHIncE pop_density lcz_class\n   <chr>                                    <int>       <dbl>     <int>\n 1 Chelsea-Hudson Yards                    118915   37463.5           2\n 2 Murray Hill-Kips Bay                    138337   29922.352         1\n 3 East Village                             72063   29772.838         2\n 4 John F. Kennedy International Airport       NA       0            15\n 5 Greenwich Village                       175436   31733.820         1\n 6 Upper West Side-Lincoln Square          158165   23416.408         1\n 7 Central Park                                NA   20753.459        12\n 8 Murray Hill-Kips Bay                    138337   44076.383         1\n 9 Upper East Side-Yorkville               133582       0             2\n10 East Harlem (North)                      35504   10275.114         2\n   lcz_label         \n   <chr>             \n 1 Compact midrise   \n 2 Compact highrise  \n 3 Compact midrise   \n 4 Bare rock or paved\n 5 Compact highrise  \n 6 Compact highrise  \n 7 Scattered trees   \n 8 Compact highrise  \n 9 Compact midrise   \n10 Compact midrise   \n# ℹ more rows\n```\n\n\n:::\n:::\n\n\nFinally, we save the data for further processing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define output path for writing final dataset\nlocations_sdf_updated_three_file_path <- file.path(\n  getwd(),\n  \"data\",\n  \"locations_sdf_updated_three\"\n)\n\n# Write enriched data back to Delta format (append mode)\nspark_write_delta(\n  locations_sdf_updated_three,\n  path = locations_sdf_updated_three_file_path,\n  mode = \"append\"\n)\n```\n:::\n\n\nAnd disconnect from our spark instance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Disconnect Spark session to free resources\nspark_disconnect(sc)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}