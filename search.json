[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Apache Sedona R Tutorial",
    "section": "",
    "text": "Hi, my name is Rodgers Iradukunda, a PhD student at the University of Liverpool and part of the Geographic Data Science Lab. I decided to put together this tutorial for R users (R-ddicts?) who, like me, sometimes use geospatial data that is too large to be processed by conventional geospatial libraries, such as sf and raster, which do not natively support distributed programming. Cue in Sparklyr and Apache Sedona, two R libraries that leverage distributed programming to solve this issue. When it comes to programming, I believe in learning by doing, hence this tutorial.\nMost cases where I have seen people struggle with processing big data involve raw data that comes in coordinate form, i.e., lat/ long pairs. For my master’s dissertation, I worked with GPS mobile data that was way larger than any other datasets I had worked with previously. While GPS coordinates are useful, they often need to be augmented with other data to really provide insights. For instance, you often want to determine the administrative area a coordinate pair belongs to. The problem, however, is handling that much data is often untenable when using conventional R libraries. Sparklyr and Apache Sedona offer a solution.\nTo replicate the problem, I used the popular New York City (NYC) taxi dataset that is openly available and used to predict trip durations based on certain variables. The main geographical information provided are pickup and dropoff coordinates. In this tutorial, I focus on using these coordinates to obtain neighbourhood information, median household income, population density, and local climatic zones. In the process, I demonstrate how one would go about processing big data using vector and raster data in R. Another underrated (imo) feauture that I highlight is Delta Lake, a storage system that makes dealing with big data to be more efficient than using more popular CSVs. I did not find much on setting up Sparklyr, Apache Sedona, and Delta Lake in R, and so I hope this this tutorial will address that gap.\nAs a novice in using these tools, I may have made some poor design choices. If you notice any, please let me know so that I make the necessary corrections.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Processing Large Datasets with Delta Lake, Sparklyr, and Apache Sedona in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html",
    "href": "preprocessing_one.html",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "",
    "text": "2.1 Introduction\nIn this chapter, we shall demonstrate how to perform basic data cleaning and feature engineering using Sparklyr, and how to save the data in the Delta Lake format.\nThe primary dataset was downloaded from Kaggle here. It provides information about taxi trips, including the pickup and dropoff times and locations. Our goal is to enrich it using additional data obtained from geospatial sources, and leave the rest to you to visualise it and perform analysis that predicts taxi trip durations.\nThe overarching goal is to show you how to go about using Delta Lake, Sparklyr, Apache Sedona, and R for big data geospatial analysis when you only have an ordinary computer at your disposal.\nFor my actual analysis, I used the entire 7.4 GB dataset provided, containing about 48 million rows. However, for this published tutorial, I sometimes use less data so as to timely publish and update this website. For reference, I am using an M1 MacBook with 16 GB of RAM and 500 GB of disk space.\nIf you have 8 GB of RAM, I would suggest that you use one of the four datasets available, as they are also relatively massive with about 12 million rows each!\nAnyhow, enough talking — let us get to work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#installing-and-loading-packages",
    "href": "preprocessing_one.html#installing-and-loading-packages",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.2 Installing and loading packages",
    "text": "2.2 Installing and loading packages\nWe shall start by installing and loading the necessary libraries: arrow, sparklyr, and dplyr.\n\ninstall.packages(\"arrow\")\ninstall.packages(\"sparklyr\")\n\nWe use sparklyr to interface with Apache Spark in R, allowing us to work efficiently with large datasets using distributed computing. The dplyr package provides powerful data manipulation functions that integrate seamlessly with Spark, making it easier to transform and summarise data. Finally, we load arrow, which enhances Spark’s performance when copying, collecting, and transforming data, thereby improving the overall efficiency of our analysis.\n\n# Load required libraries\nlibrary(arrow)      # Handle efficient data exchange between R and Spark\nlibrary(sparklyr)   # Spark connection and data manipulation\nlibrary(dplyr)      # Data manipulation functions\n\nWe can now use sparklyr to download and install Spark. In this tutorial, we shall install Spark version 3.5.5 and create the JAVA_HOME and SPARK_HOME environment variables. Although you can initialise these variables system-wide, it is often easier to set them within your working file, especially if you have multiple installations of Spark and Java on your system.\nWhilst Spark 3.5 is compiled for Java 8, 11, and 17, I would recommend using Java 11, as it has proven to be more stable in my experience compared to the other two versions. Additionally, I cannot overemphasise the importance of downloading a Java version that is natively designed for your system’s processor. As previously stated, I own an M1 MacBook (ARM 64-bit), so I previously encountered memory issues when using Java designed for Intel processors (x86 64-bit) as the processor was forced to perform extra avoidable work. Switching to an ARM-based Java greatly improved the performance of my code.\nYou can freely download an appropriate Java version for your machine here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#installing-spark-and-setting-environment-variables",
    "href": "preprocessing_one.html#installing-spark-and-setting-environment-variables",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.3 Installing Spark and setting environment variables",
    "text": "2.3 Installing Spark and setting environment variables\n\n# Install and set up Spark environment\nspark_install(\"3.5.5\")  # Install the specific version of Spark (3.5.5)\n\n# Set Java and Spark home directory paths\nSys.setenv(\"JAVA_HOME\"=\"/Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home\")  # Set Java home directory for Spark\nSys.setenv(\"SPARK_HOME\"=spark_home_dir(version = \"3.5.5\"))  # Set Spark home directory",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#configuring-spark",
    "href": "preprocessing_one.html#configuring-spark",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.4 Configuring Spark",
    "text": "2.4 Configuring Spark\nWe shall now create a folder where Spark will store temporary files. By default, Spark stores these files in memory, but in our case, we want them to be stored on disk. This is why we specify a spark_dir path to direct Spark to use disk storage for its temporary file storage.\n\n# Define path for Spark data\nspark_dir &lt;- file.path(getwd(), \"data\", \"spark\")\n\nWe now initialise a list and provide configuration settings for Spark. This is arguably one of the most important steps, as it determines both how fast your data is processed and whether it is successfully processed. A typical Spark process involves reading data from files (for instance), processing it, transmitting it between executors, and then writing it back to files. All of this is made possible by serialising and deserialising the data into bytes. Naturally, your choice of serializer will heavily influence the performance of your application. Here, we use Kryo serialisation, as it is “significantly faster and more compact than Java serialisation” (source).\nSpark runs on the Java Virtual Machine (JVM), and Java heap space refers to the memory allocated to the JVM during runtime for storing objects and data. The heap memory is divided into Spark memory (M), reserved memory, and user memory. Spark memory itself is divided into two parts: execution and storage (R). Execution memory is used for computations such as shuffles, joins, sorts, and aggregations. Storage memory, on the other hand, is used for caching and propagating internal data across the cluster (when running in cluster mode). Read more about this here.\nIn our case, since we are running our code in local mode, we set the JVM heap space to 10GB using sparklyr.shell.driver-memory. We then allocate 70% of the JVM heap space to Spark memory (M) using the spark.memory.fraction option. This means 7GB is reserved for both storage and execution. By default, 50% of M (i.e., 3.5GB) is reserved for storage (R). Although this can be adjusted using spark.memory.storageFraction, we leave it at the default here. Importantly, when no execution memory is needed, R can make use of the entire 7GB.\nOther configuration choices we make include enabling the storage of 2GB of data off-heap (i.e., outside the JVM) using the settings spark.memory.offHeap.enabled = \"true\" and spark.memory.offHeap.size = \"2g\". We also instruct Spark not to write intermediate shuffle data to disk—to avoid I/O bottlenecks—by setting spark.sql.shuffle.spill = \"false\".\nTo manage memory efficiently, we enable periodic garbage collection every 60 seconds with spark.cleaner.periodicGC.interval = \"60s\", which helps reclaim unused space. Additionally, we set our maximum partition file size to 200MB. It is recommended to keep this between 128MB and 200MB, depending on your dataset size and cluster resources (source).\nFinally, we enable Adaptive Query Execution (AQE), which allows Spark to automatically optimise query plans during runtime, such as when performing joins, thereby improving performance without manual interference (source).\nPlease update the configuration settings based on your available RAM.\n\n# Create an empty list for Spark configuration settings\nconfig &lt;- list()\n\n# Set Spark configurations for memory and performance optimisation\n\n# Use KryoSerializer for better performance\nconfig$spark.serializer &lt;- \"org.apache.spark.serializer.KryoSerializer\"  \n\n# Set temporary directory for Spark\nconfig$`sparklyr.shell.driver-java-options` &lt;- paste0(\"-Djava.io.tmpdir=\", spark_dir)  \n\n# Use compressed Oops for JVM performance\nconfig$`sparklyr.shell.driver-java-options` &lt;- \"-XX:+UseCompressedOops\"  \n\n# Allocate 10GB of memory for the Spark driver\nconfig$`sparklyr.shell.driver-memory` &lt;- '10G'  \n\n# Set fraction of heap memory used for Spark storage\nconfig$spark.memory.fraction &lt;- 0.7  \n\n# Set shuffle partitions (local setting based on workload)\nconfig$spark.sql.shuffle.partitions.local &lt;- 24  \n\n# Set extra memory for driver\nconfig$spark.driver.extraJavaOptions &lt;- \"-Xmx1G\"  \n\n# Enable off-heap memory usage\nconfig$spark.memory.offHeap.enabled &lt;- \"true\" \n\n# Set 4GB for off-heap memory\nconfig$spark.memory.offHeap.size &lt;- \"2g\"  \n\n# Disable shuffle spill to disk\nconfig$spark.sql.shuffle.spill &lt;- \"false\"  \n\n# Periodic garbage collection interval\nconfig$spark.cleaner.periodicGC.interval &lt;- \"60s\"  \n\n# Set max partition size for shuffle files\nconfig$spark.sql.files.maxPartitionBytes &lt;- \"200m\"  \n\n# Enable adaptive query execution\nconfig$spark.sql.adaptive.enabled &lt;- \"true\"  \n\nAfter configuring our setup, we now connect to Spark. Note that we have also instructed Spark to install the Delta package. This is a necessary step if you want to read from or write to Delta tables, which are commonly used for managing large-scale data with ACID transaction support among many other advantages. By including local[*] in our spark context, we have told Spark to use all available cores in our computer. If, for instance, you only wanted to use 4, you would change this to local[4].\n\n# Connect to Spark with the specified configurations\nsc &lt;- spark_connect(\n  master = \"local[*]\",  # Use all available cores for local execution\n  config = config,      # Use the specified configurations\n  packages = \"delta\"    # Install the Delta Lake package for optimised storage\n)\n\nI recommend using the Spark Web User Interface (UI) to track metrics associated with your Spark application. You can access it as shown below.\n\n# Open Spark web UI for monitoring the connection\nspark_web(sc)\n\nAfter successfully setting up a Spark context, we now turn to loading our data. We start by specifying the path where the files are located. Note that we are instructing Spark to read all CSV files within the yellow_tripdata subfolder.\nAdditionally, we organise our data into 24 partitions. We chose 24 because it is three times the number of our total cores (8). This approach helps ensure parallelism during processing and prevents data skew, which could otherwise slow down our computations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#loading-the-data",
    "href": "preprocessing_one.html#loading-the-data",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.5 Loading the data",
    "text": "2.5 Loading the data\n\n# Define the path for the yellow cab data\nyellow_cab_parent_folder &lt;- file.path(getwd(), \"data\", \"yellow_tripdata\")\nyellow_cab_filepattern &lt;- file.path(yellow_cab_parent_folder, \"*csv\")\n\n# Read the yellow cab data from CSV files into a Spark DataFrame\nyellow_cab_sdf &lt;- spark_read_csv(\n  sc, \n  path = yellow_cab_filepattern, \n  name = \"yellow_cab_sdf\"\n  ) %&gt;% \n    sdf_repartition(24)\n\n# Print the structure of the DataFrame for inspection\nprint(yellow_cab_sdf, width = Inf)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`sparklyr_tmp_33d280d0_2434_4c0f_b222_7dce9f25ef6b`&gt; [?? x 19]\n# Database: spark_connection\n   VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n   &lt;chr&gt;    &lt;chr&gt;                &lt;chr&gt;                 &lt;chr&gt;          \n 1 1        2015-01-26 12:25:20  2015-01-26 12:39:02   1              \n 2 2        2015-01-15 22:08:32  2015-01-15 22:23:12   1              \n 3 1        2015-01-27 11:30:29  2015-01-27 11:34:35   1              \n 4 1        2015-01-06 22:48:59  2015-01-06 22:59:12   1              \n 5 1        2015-01-16 21:28:01  2015-01-16 21:58:08   2              \n 6 2        2015-01-09 10:10:06  2015-01-09 10:17:24   2              \n 7 2        2015-01-24 00:43:49  2015-01-24 00:45:52   1              \n 8 1        2015-01-18 16:27:09  2015-01-18 16:35:31   2              \n 9 1        2015-01-14 22:57:21  2015-01-14 23:10:52   1              \n10 2        2015-01-02 02:09:53  2015-01-02 02:13:07   1              \n   trip_distance pickup_longitude    pickup_latitude    RateCodeID\n   &lt;chr&gt;         &lt;chr&gt;               &lt;chr&gt;              &lt;chr&gt;     \n 1 1.60          -73.988983154296875 40.748603820800781 1         \n 2 1.51          -73.994148254394531 40.751190185546875 1         \n 3 .60           -73.902961730957031 40.770702362060547 1         \n 4 1.80          -73.9700927734375   40.757877349853516 1         \n 5 7.40          -73.99237060546875  40.742774963378906 1         \n 6 1.15          -73.978851318359375 40.766895294189453 1         \n 7 .60           -73.987869262695312 40.749561309814453 1         \n 8 1.00          -73.991905212402344 40.755977630615234 1         \n 9 1.50          -73.9891357421875   40.758594512939453 1         \n10 .72           -73.978500366210938 40.783191680908203 1         \n   store_and_fwd_flag dropoff_longitude   dropoff_latitude   payment_type\n   &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;              &lt;chr&gt;       \n 1 N                  -73.98883056640625  40.740432739257813 1           \n 2 N                  -73.972984313964844 40.750438690185547 2           \n 3 N                  -73.911933898925781 40.775138854980469 1           \n 4 N                  -73.994132995605469 40.751636505126953 1           \n 5 N                  -73.908111572265625 40.708057403564453 1           \n 6 N                  -73.979690551757812 40.781406402587891 1           \n 7 N                  -73.985969543457031 40.756549835205078 2           \n 8 N                  -73.98345947265625  40.757297515869141 2           \n 9 N                  -73.973533630371094 40.747013092041016 1           \n10 N                  -73.971961975097656 40.783878326416016 2           \n   fare_amount extra mta_tax tip_amount tolls_amount improvement_surcharge\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;                \n 1 9.5         0     0.5     2.05       0            0.3                  \n 2 11          0.5   0.5     0          0            0.3                  \n 3 4.5         0     0.5     1.55       0            0.3                  \n 4 8.5         0.5   0.5     1.95       0            0.3                  \n 5 26.5        0.5   0.5     5.55       0            0.3                  \n 6 7           0     0.5     1.4        0            0.3                  \n 7 4           0.5   0.5     0          0            0.3                  \n 8 7           0     0.5     0          0            0.3                  \n 9 10          0.5   0.5     1          0            0.3                  \n10 4.5         0.5   0.5     0          0            0.3                  \n   total_amount\n   &lt;chr&gt;       \n 1 12.35       \n 2 12.3        \n 3 6.85        \n 4 11.75       \n 5 33.35       \n 6 9.2         \n 7 5.3         \n 8 7.8         \n 9 12.3        \n10 5.8         \n# ℹ more rows\n\n\nLooking at the number of partitions, we see that each core will be responsible for an approximate equal number of rows for each task. This ensures that all cores are doing an equal amount of work, without any being overworked.\n\n# Number of rows per each partition\nyellow_cab_sdf %&gt;% \n  sdf_partition_sizes()\n\n   partition_index partition_size\n1                0        1968700\n2                1        1968700\n3                2        1968701\n4                3        1968700\n5                4        1968701\n6                5        1968703\n7                6        1968702\n8                7        1968702\n9                8        1968701\n10               9        1968703\n11              10        1968706\n12              11        1968704\n13              12        1968703\n14              13        1968703\n15              14        1968701\n16              15        1968700\n17              16        1968701\n18              17        1968702\n19              18        1968701\n20              19        1968701\n21              20        1968700\n22              21        1968703\n23              22        1968705\n24              23        1968702\n\n\nBelow we can see how many columns and rows our data has.\n\n# Get the number of rows and columns in the DataFrame\nsdf_ncol(yellow_cab_sdf)\n\n[1] 19\n\nsdf_nrow(yellow_cab_sdf)\n\n[1] 47248845",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#preprocessing",
    "href": "preprocessing_one.html#preprocessing",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.6 Preprocessing",
    "text": "2.6 Preprocessing\n\n2.6.1 Updating the schema\nDepending on how much data you loaded, you may find that all the variables are in character format. This is not ideal, both for processing and memory allocation, as strings take up a significant amount of space.\n\n# Print the schema (column types) of the DataFrame\nsdf_schema(yellow_cab_sdf)\n\nWe shall, therefore, update the schema accordingly.\n\n# Data cleaning: Convert columns to appropriate types\nyellow_cab_sdf &lt;- yellow_cab_sdf |&gt;\n  mutate(\n    VendorID = as.integer(VendorID),  # Convert VendorID to integer\n    tpep_pickup_datetime = to_timestamp(tpep_pickup_datetime),  # Convert to timestamp\n    tpep_dropoff_datetime = to_timestamp(tpep_dropoff_datetime),  # Convert to timestamp\n    passenger_count = as.integer(passenger_count),  # Convert to integer\n    trip_distance = as.numeric(trip_distance),  # Convert to numeric\n    pickup_longitude = as.numeric(pickup_longitude),  # Convert to numeric\n    pickup_latitude = as.numeric(pickup_latitude),  # Convert to numeric\n    RateCodeID = as.character(RateCodeID),  # Convert to character\n    store_and_fwd_flag = as.character(store_and_fwd_flag),  # Convert to character\n    dropoff_longitude = as.numeric(dropoff_longitude),  # Convert to numeric\n    dropoff_latitude = as.numeric(dropoff_latitude),  # Convert to numeric\n    payment_type = as.character(payment_type),  # Convert to character\n    fare_amount = as.numeric(fare_amount),  # Convert to numeric\n    extra = as.numeric(extra),  # Convert to numeric\n    mta_tax = as.numeric(mta_tax),  # Convert to numeric\n    tip_amount = as.numeric(tip_amount),  # Convert to numeric\n    tolls_amount = as.numeric(tolls_amount),  # Convert to numeric\n    improvement_surcharge = as.numeric(improvement_surcharge),  # Convert to numeric\n    total_amount = as.numeric(total_amount)  # Convert to numeric\n  )\n\n\n\n2.6.2 Missing values\nWe now want to check if we have any missing values. By calling collect(), we are triggering an action. By default, Spark performs lazy evaluation, meaning it does not execute every line of code immediately. The code is only executed when actions are performed, such as collect() and count(). Learn more about this here.\nBy calling collect(), we will change the class of the resulting object into an R dataframe rather than a Spark dataframe.\n\n# Handle missing values: Summarise the missing values in each column\nmissing_values_by_col &lt;- yellow_cab_sdf |&gt;\n  summarise_all(~ sum(as.integer(is.na(.)))) |&gt;\n  collect()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n# Print missing values summary\nprint(missing_values_by_col, width = Inf)\n\n# A tibble: 1 × 19\n  VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n     &lt;int&gt;                &lt;int&gt;                 &lt;int&gt;           &lt;int&gt;\n1        0                    0                     0               0\n  trip_distance pickup_longitude pickup_latitude RateCodeID store_and_fwd_flag\n          &lt;int&gt;            &lt;int&gt;           &lt;int&gt;      &lt;int&gt;              &lt;int&gt;\n1             0                0               0          0                  0\n  dropoff_longitude dropoff_latitude payment_type fare_amount extra mta_tax\n              &lt;int&gt;            &lt;int&gt;        &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n1                 0                0            0           0     0       0\n  tip_amount tolls_amount improvement_surcharge total_amount\n       &lt;int&gt;        &lt;int&gt;                 &lt;int&gt;        &lt;int&gt;\n1          0            0                     3            0\n\n\n\n# print classes of yellow_cab_sdf and missing_values_by_col\nprint(yellow_cab_sdf %&gt;% class())\n\n[1] \"tbl_spark\" \"tbl_sql\"   \"tbl_lazy\"  \"tbl\"      \n\nprint(missing_values_by_col %&gt;% class())\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe can see that the only column with missing values is improvement_surcharge. We shall impute the missing data using the median value of the column and create a new column called improvement_surcharge_imputed.\n\n# Impute missing values for specific columns (e.g., \"improvement_surcharge\")\ninput_cols &lt;- c(\"improvement_surcharge\")\noutput_cols &lt;- paste0(input_cols, \"_imputed\")\n\nyellow_cab_sdf &lt;- yellow_cab_sdf |&gt;\n  ft_imputer(input_cols = input_cols,   # Specify input columns\n             output_cols = output_cols,  # Specify output columns\n             strategy = \"median\")  # Use median strategy for imputation\n\n\n\n2.6.3 Duplicates\nWe shall now remove duplicates based on specific columns.\n\n# Remove duplicate rows based on specific columns\nyellow_cab_sdf &lt;- sdf_drop_duplicates(\n  yellow_cab_sdf,\n  cols = c(\n    \"VendorID\",\n    \"tpep_pickup_datetime\",\n    \"tpep_dropoff_datetime\",\n    \"pickup_longitude\",\n    \"pickup_latitude\",\n    \"dropoff_longitude\",\n    \"dropoff_latitude\"\n  )\n)\n\n\n\n2.6.4 Outliers\nWe shall also handle outliers by filtering out unreasonable values in our dataset.\n\n# Handle outliers by filtering unreasonable values in columns\nsummary_stats &lt;- sdf_describe(\n  yellow_cab_sdf,\n  cols = c(\n    \"passenger_count\",\n    \"trip_distance\",\n    \"fare_amount\",\n    \"total_amount\"\n  )\n) |&gt;\n  collect()\n\nprint(summary_stats, width=Inf)\n\n# A tibble: 5 × 5\n  summary passenger_count    trip_distance     fare_amount       \n  &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;             &lt;chr&gt;             \n1 count   47231381           47231381          47231381          \n2 mean    1.6669565304474159 7.511100940283872 12.396603992375454\n3 stddev  1.3220203537930795 6488.8576466189   78.63046129746266 \n4 min     0                  -3390583.8        -450.0            \n5 max     9                  1.90726288E7      429496.72         \n  total_amount      \n  &lt;chr&gt;             \n1 47231381          \n2 15.598314385358897\n3 580.2462379599986 \n4 -450.3            \n5 3950611.6         \n\n\n\n# Filter out outliers based on summary statistics\nyellow_cab_sdf &lt;- yellow_cab_sdf |&gt;\n  filter(fare_amount &gt; 0 & fare_amount &lt;= 1000,\n         trip_distance &gt; 0 & trip_distance &lt; 100)\n\n\n\n2.6.5 Feauture Engineering\nThis is followed by performing feature engineering, where we derive certain columns such as the hour, day, week, and month of pickup and dropoff. We also derive variables indicating whether the pickup and dropoff occurred on a weekend and whether the pickup was during rush hour.\n\n# Feature Engineering: Create new time-based features (pickup and dropoff times)\nyellow_cab_sdf &lt;- yellow_cab_sdf |&gt;\n  mutate(\n    pickup_hour = hour(tpep_pickup_datetime),  # Hour of the pickup\n    pickup_dayofweek = date_format(tpep_pickup_datetime, \"E\"),  # Day of the week for pickup\n    pickup_week = weekofyear(tpep_pickup_datetime),  # Week of the year for pickup\n    pickup_month = month(tpep_pickup_datetime),  # Month of pickup\n    dropoff_hour = hour(tpep_dropoff_datetime),  # Hour of the dropoff\n    dropoff_dayofweek = date_format(tpep_pickup_datetime, \"E\"),  # Day of the week for dropoff\n    dropoff_week = weekofyear(tpep_dropoff_datetime),  # Week of the year for dropoff\n    dropoff_month = month(tpep_dropoff_datetime),  # Month of dropoff\n    is_weekend_pickup = ifelse(pickup_dayofweek %in% c(\"Sat\", \"Sun\"), 1, 0),  # Weekend pickup flag\n    is_weekend_dropoff = ifelse(dropoff_dayofweek %in% c(\"Sat\", \"Sun\"), 1, 0),  # Weekend dropoff flag\n    is_rush_hour_pickup = ifelse(pickup_hour %in% c(7:9, 16:19), 1, 0)  # Rush hour pickup flag\n  )\n\n\n\n2.6.6 Standardisation\nWe now normalise trip_distance and fare_amount to standardise our data for modelling.\n\n# Normalise features to standardise data for machine learning\nyellow_cab_sdf &lt;- yellow_cab_sdf %&gt;%\n  mutate(\n    trip_distance_scaled = (trip_distance - mean(trip_distance)) / sd(trip_distance),  # Standardise trip distance\n    fare_amount_scaled = (fare_amount - mean(fare_amount)) / sd(fare_amount)  # Standardise fare amount\n  )\n\n# Print the first 5 rows of the updated data\nprint(yellow_cab_sdf, n=5, width = Inf)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   SQL [?? x 33]\n# Database: spark_connection\n  VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n     &lt;int&gt; &lt;dttm&gt;               &lt;dttm&gt;                          &lt;int&gt;\n1        2 2015-01-01 00:00:00  2015-01-01 00:00:00                 1\n2        1 2015-01-01 00:02:57  2015-01-01 00:05:36                 1\n3        1 2015-01-01 00:06:44  2015-01-01 00:07:06                 2\n4        1 2015-01-01 00:01:51  2015-01-01 00:07:07                 3\n5        1 2015-01-01 00:03:58  2015-01-01 00:07:23                 1\n  trip_distance pickup_longitude pickup_latitude RateCodeID store_and_fwd_flag\n          &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;             \n1          1.68            -74.0            40.8 1          N                 \n2          0.6             -74.0            40.8 1          N                 \n3          7.8             -74.0            40.8 1          N                 \n4          0.7             -74.0            40.7 1          N                 \n5          0.3             -74.0            40.7 1          N                 \n  dropoff_longitude dropoff_latitude payment_type fare_amount extra mta_tax\n              &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1               0                0   2                   10     0       0.5\n2             -74.0             40.8 1                    4     0.5     0.5\n3             -74.0             40.8 2                    2.5   0.5     0.5\n4             -74.0             40.7 2                    5.5   0.5     0.5\n5             -74.0             40.7 2                    4     0.5     0.5\n  tip_amount tolls_amount improvement_surcharge total_amount\n       &lt;dbl&gt;        &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n1        0              0                   0.3         10.8\n2        1.5            0                   0            6.8\n3        0              0                   0            3.8\n4        0              0                   0            6.8\n5        0              0                   0            5.3\n  improvement_surcharge_imputed pickup_hour pickup_dayofweek pickup_week\n                          &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;\n1                           0.3           0 Thu                        1\n2                           0             0 Thu                        1\n3                           0             0 Thu                        1\n4                           0             0 Thu                        1\n5                           0             0 Thu                        1\n  pickup_month dropoff_hour dropoff_dayofweek dropoff_week dropoff_month\n         &lt;int&gt;        &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt;\n1            1            0 Thu                          1             1\n2            1            0 Thu                          1             1\n3            1            0 Thu                          1             1\n4            1            0 Thu                          1             1\n5            1            0 Thu                          1             1\n  is_weekend_pickup is_weekend_dropoff is_rush_hour_pickup trip_distance_scaled\n              &lt;dbl&gt;              &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1                 0                  0                   0               -0.342\n2                 0                  0                   0               -0.648\n3                 0                  0                   0                1.39 \n4                 0                  0                   0               -0.620\n5                 0                  0                   0               -0.733\n  fare_amount_scaled\n               &lt;dbl&gt;\n1             -0.227\n2             -0.811\n3             -0.957\n4             -0.665\n5             -0.811\n# ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#separating-the-data",
    "href": "preprocessing_one.html#separating-the-data",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.7 Separating the data",
    "text": "2.7 Separating the data\nAt this point, I separate my data into two sets: location-related data and other non-location data. I do this because the next few steps involve obtaining additional geospatial variables solely based on pickup and dropoff coordinates. Instead of working with a dataset containing 20-plus columns, I will now only need four: trip_id, latitude, longitude, and is_pickup.\nThe only downside is that I will double the number of rows since pickup and dropoff coordinates for the same trip will now be in separate rows. I justify this decision because the alternative—performing heavy spatial joins twice on the same dataset—is quite resource-intensive. Another alternative would be to save the pickup and dropoff locations in separate datasets. Ultimately, you can make various design decisions based on the resources available to you.\n\n# Separate data into two parts: location and trip metadata\nyellow_cab_sdf &lt;- yellow_cab_sdf %&gt;% \n  sdf_with_unique_id(id = \"trip_id\")  # Add unique trip ID\n\n\n# Create separate DataFrames for pickup and dropoff locations\npickup_sdf &lt;- yellow_cab_sdf %&gt;% \n  transmute(\n    trip_id,\n    latitude = pickup_latitude,\n    longitude = pickup_longitude,\n    is_pickup = 1  # Flag for pickup locations\n  )\n\ndropoff_sdf &lt;- yellow_cab_sdf %&gt;% \n  transmute(\n    trip_id,\n    latitude = dropoff_latitude,\n    longitude = dropoff_longitude,\n    is_pickup = 0  # Flag for dropoff locations\n  )\n\n# Combine pickup and dropoff locations into one DataFrame\nlocations_sdf &lt;- sdf_bind_rows(\n  pickup_sdf,\n  dropoff_sdf\n)\n\nprint(locations_sdf, width = Inf, n=10)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`sparklyr_tmp__8390928b_cdb8_4fac_b1ed_c4006a9db6c2`&gt; [?? x 4]\n# Database: spark_connection\n   trip_id latitude longitude is_pickup\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     400     40.8     -74.0         1\n 2     401     40.8     -74.0         1\n 3     402     40.7     -74.0         1\n 4     403     40.7     -74.0         1\n 5     404      0         0           1\n 6     405     40.8     -74.0         1\n 7     400     40.8     -74.0         0\n 8     401     40.8     -73.9         0\n 9     402     40.7     -74.0         0\n10     403     40.7     -74.0         0\n# ℹ more rows\n\n\n\n# Create another DataFrame for non-location trip data (excluding coordinates)\ntrip_data_sdf &lt;- yellow_cab_sdf %&gt;% \n  select(\n    -c(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)  # Exclude latitude and longitude\n  )\n\nprint(trip_data_sdf, width = Inf, n=10)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   SQL [?? x 30]\n# Database: spark_connection\n  VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n     &lt;int&gt; &lt;dttm&gt;               &lt;dttm&gt;                          &lt;int&gt;\n1        2 2015-01-01 00:32:14  2015-01-01 00:40:14                 2\n2        1 2015-01-01 00:30:35  2015-01-01 00:40:15                 1\n3        1 2015-01-01 00:30:28  2015-01-01 00:40:22                 1\n4        2 2015-01-01 00:28:51  2015-01-01 00:40:22                 1\n5        1 2015-01-01 00:23:18  2015-01-01 00:40:22                 2\n6        2 2015-01-01 00:24:48  2015-01-01 00:40:23                 2\n  trip_distance RateCodeID store_and_fwd_flag payment_type fare_amount extra\n          &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;              &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1          1.82 1          N                  2                    8     0.5\n2          2    1          N                  2                    9.5   0.5\n3          4.7  1          N                  1                   14.7   0.5\n4          1.46 1          N                  1                    9.5   0.5\n5          2    1          N                  2                   11.5   0.5\n6          2.46 1          N                  1                   12     0.5\n  mta_tax tip_amount tolls_amount improvement_surcharge total_amount\n    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n1     0.5       0               0                   0.3          9.3\n2     0.5       0               0                   0           10.8\n3     0.5       3.2             0                   0           19.2\n4     0.5       3               0                   0.3         13.8\n5     0.5       0               0                   0           12.8\n6     0.5       3.75            0                   0.3         17.0\n  improvement_surcharge_imputed pickup_hour pickup_dayofweek pickup_week\n                          &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;\n1                           0.3           0 Thu                        1\n2                           0             0 Thu                        1\n3                           0             0 Thu                        1\n4                           0.3           0 Thu                        1\n5                           0             0 Thu                        1\n6                           0.3           0 Thu                        1\n  pickup_month dropoff_hour dropoff_dayofweek dropoff_week dropoff_month\n         &lt;int&gt;        &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt;\n1            1            0 Thu                          1             1\n2            1            0 Thu                          1             1\n3            1            0 Thu                          1             1\n4            1            0 Thu                          1             1\n5            1            0 Thu                          1             1\n6            1            0 Thu                          1             1\n  is_weekend_pickup is_weekend_dropoff is_rush_hour_pickup trip_distance_scaled\n              &lt;dbl&gt;              &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1                 0                  0                   0               -0.302\n2                 0                  0                   0               -0.251\n3                 0                  0                   0                0.514\n4                 0                  0                   0               -0.404\n5                 0                  0                   0               -0.251\n6                 0                  0                   0               -0.121\n  fare_amount_scaled trip_id\n               &lt;dbl&gt;   &lt;dbl&gt;\n1            -0.421      400\n2            -0.275      401\n3             0.231      402\n4            -0.275      403\n5            -0.0806     404\n6            -0.0320     405",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#writing-the-data",
    "href": "preprocessing_one.html#writing-the-data",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.8 Writing the data",
    "text": "2.8 Writing the data\nFinally, we save the preprocessed data into Delta Lake. While we had no choice in determining the format of the initial dataset, we do have a choice in how we write it. Delta Lake is based on Parquet files, but incorporates additional metadata that improves the efficiency of dealing with multiple parquet files.\nThe main difference between Parquet files and CSV files is that Parquet is columnar-based, while CSV is row-based. This offers several advantages to Parquet files, such as faster reading and smaller file sizes. Delta Lake further enhances Parquet files by adding ACID capabilities, among other features. You can find a detailed discussion of the advantages of using Delta tables over Parquet files here.\n\n# Save the location and trip data to disk using Delta Lake format\nsave_file_path_locations_sdf &lt;- file.path(getwd(), \"data\", \"locations_sdf\")\nspark_write_delta(\n  locations_sdf,\n  save_file_path_locations_sdf,\n  mode = \"overwrite\"  # Overwrite existing file if it exists\n)\n\nsave_file_path_trip_data_sdf &lt;- file.path(getwd(), \"data\", \"trip_data_sdf\")\nspark_write_delta(\n  trip_data_sdf,\n  save_file_path_trip_data_sdf,\n  mode = \"overwrite\"  # Overwrite existing file if it exists\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_one.html#disconnecting-spark-context",
    "href": "preprocessing_one.html#disconnecting-spark-context",
    "title": "2  Preprocessing and Feature Engineering for Yellow Cab Trip Data",
    "section": "2.9 Disconnecting Spark context",
    "text": "2.9 Disconnecting Spark context\nFinally, we disconnect from our Spark context to release the memory being held by Spark.\n\n# Disconnect from Spark session\nspark_disconnect(sc)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocessing and Feature Engineering for Yellow Cab Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html",
    "href": "preprocessing_two.html",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "",
    "text": "3.1 Introduction\nIn this part, we are going to use the saved locations data to determine the pickup and dropoff neighbourhoods associated with each trip. We shall then use these additional data to obtain the median household incomes of the pickup and dropoff neighbourhoods. We use income as a proxy for affluence. Humour me by assuming that there is a relationship between the duration of a taxi trip and the affluence of either or both the pickup or dropoff locations.\nWe shall introduce two new datasets: NYC Neighbourhood Tabulation Areas (NTAs) boundaries based on the 2020 census, and NTA household median income based on the 2022 American Community Survey (ACS). The boundaries data is in vector format, while the income data is in CSV format (originally .xlsx but filtered for GeoID and MdHHIncE columns, then saved as CSV). To do this, we shall use Apache Sedona (v1.7.0) to merge the NTA boundaries and income data. We will then perform a spatial join on the coordinates with the boundaries, determining the pickup and dropoff neighbourhoods.\nBy the way, this will be a new file and not the same as the one used in Chapter 2. When working in local mode, I have found that it is more feasible to separate preprocessing into multiple stages. Running too many transformations at once is likely to result in out-of-memory errors and take a very long time to complete.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#installing-and-loading-packages",
    "href": "preprocessing_two.html#installing-and-loading-packages",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.2 Installing and loading packages",
    "text": "3.2 Installing and loading packages\n\ninstall.packages(\"apache.sedona\")\n\n\n# Load necessary libraries for Spark, geospatial data, and data manipulation\nlibrary(arrow)\nlibrary(sparklyr)\nlibrary(sf)\nlibrary(dplyr)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#configuring-spark",
    "href": "preprocessing_two.html#configuring-spark",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.3 Configuring Spark",
    "text": "3.3 Configuring Spark\n\n# Define the Spark directory for temporary files\nspark_dir &lt;- file.path(getwd(), \"data\", \"spark\")\n\nThe configuration will mostly be kept similar to the one in the first file. The main difference is that some of the Delta configurations are explicitly included and not added as a package. This is because Delta and Apache Sedona clash when Delta is installed as a package when creating the Spark context.\nYou will notice a few differences in how the Spark context is set up this time around. Be sure to use this type of setup when working with Apache Sedona and reading or writing to Delta Lake.\n\n# Create an empty list for Spark configuration settings\nconfig &lt;- list()\n\n# Set Spark configurations for memory and performance optimisation\n\n# Configure some delta specific options\nconfig$spark.sql.extensions &lt;- \"io.delta.sql.DeltaSparkSessionExtension\"\nconfig$spark.sql.catalog.spark_catalog &lt;- \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n\n# Use KryoSerializer for better performance\nconfig$spark.serializer &lt;- \"org.apache.spark.serializer.KryoSerializer\"  \n\n# Set temporary directory for Spark\nconfig$`sparklyr.shell.driver-java-options` &lt;- paste0(\"-Djava.io.tmpdir=\", spark_dir)  \n\n# Use compressed Oops for JVM performance\nconfig$`sparklyr.shell.driver-java-options` &lt;- \"-XX:+UseCompressedOops\"  \n\n# Allocate 8GB of memory for the Spark driver\nconfig$`sparklyr.shell.driver-memory` &lt;- '10G'  \n\n# Set fraction of heap memory used for Spark storage\nconfig$spark.memory.fraction &lt;- 0.7  \n\n# Set shuffle partitions (local setting based on workload)\nconfig$spark.sql.shuffle.partitions.local &lt;- 24  \n\n# Set extra memory for driver\nconfig$spark.driver.extraJavaOptions &lt;- \"-Xmx1G\"  \n\n# Enable off-heap memory usage\nconfig$spark.memory.offHeap.enabled &lt;- \"true\" \n\n# Set 4GB for off-heap memory\nconfig$spark.memory.offHeap.size &lt;- \"2g\"  \n\n# Disable shuffle spill to disk\nconfig$spark.sql.shuffle.spill &lt;- \"false\"  \n\n# Periodic garbage collection interval\nconfig$spark.cleaner.periodicGC.interval &lt;- \"60s\"  \n\n# Set max partition size for shuffle files\nconfig$spark.sql.files.maxPartitionBytes &lt;- \"200m\"  \n\n# Enable adaptive query execution\nconfig$spark.sql.adaptive.enabled &lt;- \"true\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#instantiating-spark-context",
    "href": "preprocessing_two.html#instantiating-spark-context",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.4 Instantiating spark context",
    "text": "3.4 Instantiating spark context\nAs you can see, when initiating our Spark context, we explicitly include files associated with Delta and Apache Sedona. By doing so, Apache Sedona and Delta packages do not clash with each other.\n\n# Connect to Spark with the defined configuration and additional packages for geospatial processing\nsc &lt;- spark_connect(\n  master = \"local[*]\",\n  config = config,\n  packages = c(\n    \"io.delta:delta-spark_2.12:3.3.0\",\n    \"org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0\",\n    \"org.datasyslab:geotools-wrapper:1.7.0-28.5\"\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#loading-apache-sedona",
    "href": "preprocessing_two.html#loading-apache-sedona",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.5 Loading Apache Sedona",
    "text": "3.5 Loading Apache Sedona\nIt is only after initializing our Spark context that we can now load the Apache Sedona library and initialize its context. We are now ready to get started!\n\nlibrary(apache.sedona)\ninvoke_static(\n  sc,\n  \"org.apache.sedona.spark.SedonaContext\",\n  \"create\",\n  spark_session(sc),\n  \"r\"\n)\n\n&lt;jobj[23]&gt;\n  org.apache.spark.sql.SparkSession\n  org.apache.spark.sql.SparkSession@5bb85469\n\n\n\n# Launch Spark web UI to monitor the Spark session\nspark_web(sc)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#loading-datasets",
    "href": "preprocessing_two.html#loading-datasets",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.6 Loading datasets",
    "text": "3.6 Loading datasets\n\n3.6.1 Locations data\nWe now read our location data, which is thankfully in Delta Lake format.\n\n# Define the folder containing location data (latitude and longitude of yellow cabs)\nlocations_sdf_parent_folder &lt;- file.path(getwd(), \"data\", \"locations_sdf\")\nlocations_sdf &lt;- spark_read_delta(\n  sc, \n  path = locations_sdf_parent_folder, \n  name = \"locations_sdf\"\n  ) %&gt;% \n    sdf_repartition(24)\n\nprint(locations_sdf, width=Inf, n=10)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`sparklyr_tmp_e9d6fb30_b884_4190_84ff_59457673c057`&gt; [?? x 4]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 10626731     40.8     -74.0         1\n 2  8962259     40.7     -74.0         1\n 3  8129828     40.8     -74.0         1\n 4  7977508     40.8     -74.0         1\n 5  8775269     40.8     -73.9         1\n 6  7608894     40.8     -74.0         1\n 7  7466839     40.8     -74.0         1\n 8  3762150     40.8     -74.0         1\n 9 12333790     40.7     -74.0         1\n10   562063     40.7     -74.0         1\n# ℹ more rows\n\n\nThe data contains nearly 94 million rows!\n\n# Print the number of rows in the locations SDF (Spark DataFrame)\nsdf_nrow(locations_sdf)\n\n[1] 93889664\n\n\nAnd is partitioned equally for optimised wide transformations, especially joins. Wide transformations are those that require data shuffling (exchange) between multiple executors such as aggregations and joins. Meanwhile, narrow transformations do not require any exchange of data. Examples of narrow transformations include select and filter.\n\nlocations_sdf %&gt;% sdf_partition_sizes()\n\n   partition_index partition_size\n1                0        3912070\n2                1        3912069\n3                2        3912070\n4                3        3912070\n5                4        3912070\n6                5        3912071\n7                6        3912070\n8                7        3912070\n9                8        3912070\n10               9        3912070\n11              10        3912070\n12              11        3912070\n13              12        3912069\n14              13        3912069\n15              14        3912069\n16              15        3912069\n17              16        3912069\n18              17        3912068\n19              18        3912068\n20              19        3912068\n21              20        3912068\n22              21        3912069\n23              22        3912069\n24              23        3912069\n\n\n\n\n3.6.2 Median household income by neighbourhood data\nWe now load the average household income by neighbourhood in NYC.\n\n# Load income data (household income by NYC neighbourhood)\nnyc_nta_hh_income_file_path &lt;- file.path(getwd(), \"data\", \"nyc_nta_med_inc\", \"nyc_nta_med_inc.csv\")\nnyc_nta_hh_income &lt;- spark_read_csv(sc, path = nyc_nta_hh_income_file_path, name = \"nyc_nta_hh_income\")\n\n\n# Display the income data\nprint(nyc_nta_hh_income, width = Inf, n=10)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`nyc_nta_hh_income`&gt; [?? x 2]\n# Database: spark_connection\n   GeoID  MdHHIncE\n   &lt;chr&gt;     &lt;int&gt;\n 1 BK0101   125469\n 2 BK0102   129838\n 3 BK0103    36951\n 4 BK0104    71107\n 5 BK0201   179877\n 6 BK0202   149181\n 7 BK0203   107633\n 8 BK0204   115160\n 9 BK0301    76660\n10 BK0302    69954\n# ℹ more rows\n\n\n\n\n3.6.3 NYC neighbourhoods data\nWe also load the shapefile using Apache Sedona. Note that we point Sedona to the entire folder and not just the specific .shp file, as is the case when reading shapefiles via sf.\n\n# Load the shapefile for NYC neighbourhoods\nny_neighs_pathfile &lt;- file.path(getwd(), \"data\", \"shapefiles\", \"nynta2020_25a\")\nny_neighbourhoods_shp &lt;- spark_read_shapefile(sc, path = ny_neighs_pathfile, name = \"ny_neighbourhoods_shp\")\n\nWarning: `spark_read_shapefile()` was deprecated in apache.sedona 1.7.1.\nℹ Please use `spark_read_source()` instead.\n\n\n\n# Display a quick summary of the shapefile data\nny_neighbourhoods_shp %&gt;% glimpse()\n\nRows: ??\n\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\nColumns: 12\nDatabase: spark_connection\n$ geometry   &lt;arrw_bnr&gt; &lt;32, 00, 00, 00, db, 01, 00, 00, 00, 00, 90, fe, 67, 9…\n$ BoroCode   &lt;chr&gt; \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\",…\n$ BoroName   &lt;chr&gt; \"Brooklyn\", \"Brooklyn\", \"Brooklyn\", \"Brooklyn\", \"Brooklyn\",…\n$ CountyFIPS &lt;chr&gt; \"047\", \"047\", \"047\", \"047\", \"047\", \"047\", \"047\", \"047\", \"04…\n$ NTA2020    &lt;chr&gt; \"BK0101\", \"BK0102\", \"BK0103\", \"BK0104\", \"BK0201\", \"BK0202\",…\n$ NTAName    &lt;chr&gt; \"Greenpoint\", \"Williamsburg\", \"South Williamsburg\", \"East W…\n$ NTAAbbrev  &lt;chr&gt; \"Grnpt\", \"Wllmsbrg\", \"SWllmsbrg\", \"EWllmsbrg\", \"BkHts\", \"Dw…\n$ NTAType    &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"6\", \"0\", \"0\", \"0\",…\n$ CDTA2020   &lt;chr&gt; \"BK01\", \"BK01\", \"BK01\", \"BK01\", \"BK02\", \"BK02\", \"BK02\", \"BK…\n$ CDTAName   &lt;chr&gt; \"BK01 Williamsburg-Greenpoint (CD 1 Equivalent)\", \"BK01 Wil…\n$ Shape_Leng &lt;chr&gt; \"2.89195621509e+04\", \"2.80980268049e+04\", \"1.82502803058e+0…\n$ Shape_Area &lt;chr&gt; \"3.53218095620e+07\", \"2.88543140796e+07\", \"1.52089606156e+0…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#associating-neighbourhood-with-median-household-income",
    "href": "preprocessing_two.html#associating-neighbourhood-with-median-household-income",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.7 Associating neighbourhood with median household income",
    "text": "3.7 Associating neighbourhood with median household income\nWe now join the income and boundaries data using their common ID.\n\n# Join the neighbourhood shapefile with the income data\nny_neighbourhoods_shp &lt;- ny_neighbourhoods_shp %&gt;%\n  left_join(nyc_nta_hh_income, by = c(\"NTA2020\" = \"GeoID\"))\n\nNow, we need to determine the relevant CRS that our shapefile uses. If it differs from EPSG:4326, we must convert it so that we can match it with the pickup and dropoff coordinates. I have not found a way to determine the CRS using Apache Sedona, so I use sf for that.\n\n# Read the shapefile as an SF (Simple Features) object for geospatial operations\nny_neighs_sf &lt;- st_read(file.path(ny_neighs_pathfile, \"nynta2020.shp\"))\n\nReading layer `nynta2020' from data source \n  `/Users/rodgersiradukunda/Library/CloudStorage/OneDrive-TheUniversityofLiverpool/geospatial_docker/sedona-tutorial/data/shapefiles/nynta2020_25a/nynta2020.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 262 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 913175.1 ymin: 120128.4 xmax: 1067383 ymax: 272844.3\nProjected CRS: NAD83 / New York Long Island (ftUS)\n\nst_crs(ny_neighs_sf)\n\nCoordinate Reference System:\n  User input: NAD83 / New York Long Island (ftUS) \n  wkt:\nPROJCRS[\"NAD83 / New York Long Island (ftUS)\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 New York Long Island zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",40.1666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-74,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",41.0333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",40.6666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",984250,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - New York - counties of Bronx; Kings; Nassau; New York; Queens; Richmond; Suffolk.\"],\n        BBOX[40.47,-74.26,41.3,-71.8]],\n    ID[\"EPSG\",2263]]\n\n\nKnowing that it is EPSG:2263, we can now convert it to EPSG:4326.\n\n# Reproject the geometries to a different coordinate reference system (CRS) for consistency\nny_neighbourhoods_shp &lt;- ny_neighbourhoods_shp %&gt;%\n  mutate(\n    geometry = st_transform(\n      geometry,\n      \"epsg:2263\",  # Source CRS\n      \"epsg:4326\",  # Target CRS\n      F\n    )\n  ) %&gt;%\n  select(\n    -c(\n      BoroCode,\n      CountyFIPS,\n      NTAAbbrev,\n      NTAType,\n      CDTA2020,\n      CDTAName,\n      Shape_Leng,\n      Shape_Area\n    )\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#joining-locations-data-with-neighbourhoods-data",
    "href": "preprocessing_two.html#joining-locations-data-with-neighbourhoods-data",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.8 Joining locations data with neighbourhoods data",
    "text": "3.8 Joining locations data with neighbourhoods data\nBecause the boundaries data is very small (about 2 MB on disk), we can cache it in memory for faster access. Generally, you are encouraged to cache data that is less than 10 MB. We are also broadcasting the neighbourhoods data to improve performance. Broadcasting means that our data is shared in its entirety with every executor so it is not shuffled when joining. This reduces data transfer overhead and improves performance.\nEven if we did not explicitly broadcast our data, it most likely would have been broadcasted automatically due to Adaptive Query Execution (AQE) since we enabled it at the start using the option config$spark.sql.adaptive.enabled &lt;- \"true\". AQE finds the optimal way of conducting joins, and since our neighbourhoods data is minuscule, chances are that it would have been broadcasted to prevent unnecessary shuffling.\n\n# Persist the neighbourhood shapefile in memory for faster access\nny_neighbourhoods_shp &lt;- sdf_broadcast(ny_neighbourhoods_shp)\nsdf_persist(ny_neighbourhoods_shp, storage.level = \"MEMORY_ONLY\")\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`sparklyr_tmp_19d9eb0f_c4b5_42d6_8835_4db9f1ca3c98`&gt; [?? x 5]\n# Database: spark_connection\n# ℹ more rows\n# ℹ 5 more variables: geometry &lt;arrw_bnr&gt;, BoroName &lt;chr&gt;, NTA2020 &lt;chr&gt;,\n#   NTAName &lt;chr&gt;, MdHHIncE &lt;int&gt;\n\n\nI have found that it is best to use Spatial SQL when conducting spatial joins or any other spatial analysis using Apache Sedona functions. To do this, we first need to register our dataframes as temporary SQL views. This will be our next step.\n\n# Register the dataframes as temporary SQL views for querying\nlocations_sdf %&gt;% sdf_register(\"locations_sdf_view\")\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`locations_sdf_view`&gt; [?? x 4]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 40000037     40.7     -74.0         0\n 2 40000093     40.7     -74.0         0\n 3 40000016     40.8     -74.0         0\n 4 40000091     40.8     -74.0         0\n 5 40000100     40.8     -74.0         1\n 6 40000031     40.8     -74.0         1\n 7 40000044     40.8     -74.0         1\n 8 40000015     40.8     -74.0         1\n 9 40000005     40.7     -74.0         0\n10 40000053     40.8     -74.0         0\n# ℹ more rows\n\nny_neighbourhoods_shp %&gt;% sdf_register(\"ny_neighbourhoods_shp_view\")\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`ny_neighbourhoods_shp_view`&gt; [?? x 5]\n# Database: spark_connection\n# ℹ more rows\n# ℹ 5 more variables: geometry &lt;arrw_bnr&gt;, BoroName &lt;chr&gt;, NTA2020 &lt;chr&gt;,\n#   NTAName &lt;chr&gt;, MdHHIncE &lt;int&gt;\n\n\nUpon registration, we can now conduct a spatial join, asking Apache Sedona to find neighbourhoods that contain specific coordinates using the ST_Contains function. You can find documentation on all available Apache Sedona vector functions here.\n\n# Perform a spatial join to associate each location (latitude, longitude) with the corresponding neighbourhood\nlocations_sdf_updated &lt;- sdf_sql(\n  sc,\n  \"\n  SELECT /*+ BROADCAST(b) */ a.*, b.*\n  FROM locations_sdf_view a\n  LEFT JOIN ny_neighbourhoods_shp_view b\n    ON ST_Contains(b.geometry, ST_Point(a.longitude, a.latitude))\n  \"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_two.html#writing-data-to-disk",
    "href": "preprocessing_two.html#writing-data-to-disk",
    "title": "3  Processing Vector Data with Apache Sedona and Sparklyr in R",
    "section": "3.9 Writing data to disk",
    "text": "3.9 Writing data to disk\nBefore saving our updated data, we remove the geometry column, as Delta Lake does not support geometry columns. Moreover, there is no need to keep it in the data, as we don’t plan on mapping the data just yet. If you need to write big data geometry files, consider using the GeoParquet format. You can do so using Spark’s spark_write_geoparquet function or the spark_write_source function with the mode set to “geoparquet”.\n\n# Remove the geometry column from the final dataset for further analysis\nlocations_sdf_updated_no_geom &lt;- locations_sdf_updated %&gt;%\n  select(-c(geometry))\n\nOur final data is as shown below. Not too bad for the few lines of code written.\n\n# Print the updated data with all relevant fields (no geometry)\nwithr::with_options(\n  list(pillar.sigfig = 6),\n  print(locations_sdf_updated_no_geom, n=10)\n)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   SQL [?? x 8]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup BoroName  NTA2020 NTAName      MdHHIncE\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;           &lt;int&gt;\n 1 40000037  40.7113  -74.0107         0 Manhattan MN0101  Financial D…   195153\n 2 40000093  40.7401  -73.9860         0 Manhattan MN0602  Gramercy       155905\n 3 40000016  40.8148  -73.9553         0 Manhattan MN0902  Manhattanvi…    46367\n 4 40000091  40.7743  -73.9614         0 Manhattan MN0802  Upper East …   194910\n 5 40000100  40.7604  -73.9614         1 Manhattan MN0801  Upper East …   133349\n 6 40000031  40.7794  -73.9849         1 Manhattan MN0701  Upper West …   158165\n 7 40000044  40.7633  -73.9594         1 Manhattan MN0801  Upper East …   133349\n 8 40000015  40.7776  -73.9551         1 Manhattan MN0802  Upper East …   194910\n 9 40000005  40.7419  -73.9746         0 Manhattan MN0603  Murray Hill…   138337\n10 40000053  40.7765  -73.9767         0 Manhattan MN0701  Upper West …   158165\n# ℹ more rows\n\nlocations_sdf_updated_no_geom %&gt;% glimpse()\n\nRows: ??\n\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\nColumns: 8\nDatabase: spark_connection\n$ trip_id   &lt;dbl&gt; 40000037, 40000093, 40000016, 40000091, 40000100, 40000031, …\n$ latitude  &lt;dbl&gt; 40.71130, 40.74007, 40.81481, 40.77428, 40.76038, 40.77941, …\n$ longitude &lt;dbl&gt; -74.01067, -73.98603, -73.95528, -73.96144, -73.96136, -73.9…\n$ is_pickup &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, …\n$ BoroName  &lt;chr&gt; \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manhatt…\n$ NTA2020   &lt;chr&gt; \"MN0101\", \"MN0602\", \"MN0902\", \"MN0802\", \"MN0801\", \"MN0701\", …\n$ NTAName   &lt;chr&gt; \"Financial District-Battery Park City\", \"Gramercy\", \"Manhatt…\n$ MdHHIncE  &lt;int&gt; 195153, 155905, 46367, 194910, 133349, 158165, 133349, 19491…\n\n\nWe now write our data to Delta Lake format as usual.\n\n# Define the file path for saving the updated dataset\nsave_locations_sdf_updated_one_filepath &lt;- file.path(getwd(), \"data\", \"locations_sdf_updated_one\")\n\n# Save the updated dataset to Delta format\nspark_write_delta(\n  locations_sdf_updated_no_geom,\n  path = save_locations_sdf_updated_one_filepath\n)\n\nAnd disconnect our spark instance.\n\n# Disconnect from the Spark session once done\nspark_disconnect(sc)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processing Vector Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_three.html",
    "href": "preprocessing_three.html",
    "title": "4  Part One - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "",
    "text": "4.1 Introduction\nIn this chapter, we are going to demonstrate how to obtain information from raster files based on geographic coordinates. Let us assume that there is a relationship between the duration of a taxi (dependent variable) ride and the population density of an area. We will, therefore, need to extract population density values at each pickup and dropoff location. Such granular data is typically available in raster format, which is why we use WorldPop population density data with a resolution of 1 km by 1 km for this purpose. You can download the data here to follow along.\nThe Spark configuration used in this chapter — and the next one — is identical to the one used in Chapter 3, so it is not shown below for brevity’s sake.\nFurthermore, because I need to re-run this code multiple times to render this website, I shall filter only a few rows from the 94 million we previously worked with, as I will be constantly updating this site. In the actual analysis I conducted, however, I used the full dataset.\nYou can find out more about using Apache Sedona for raster manipulation here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part One - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_three.html#loading-updated-locations-data",
    "href": "preprocessing_three.html#loading-updated-locations-data",
    "title": "4  Part One - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "4.2 Loading updated locations data",
    "text": "4.2 Loading updated locations data\nWe start by loading our updated locations data, which now contains household median income by neighbourhood information.\nTo reiterate, I will filter for a few rows here so that I can render this webpage faster. Sometimes in your analysis, you will find that you have too much data to fit in memory, especially when running complex transformations. In such cases, you can filter for specific rows, perform your analysis on that subset, and then append the results to your delta tables. You can repeat this process for another set of rows until you are done.\nFor instance, knowing that I have trip ID values ranging from 0 to about 48,000,000, I would:\n\nFirst filter for rows between 0 and 16 million,\n\nThen 16 million to 32 million,\n\nAnd finally, anything above 32 million,\n\nAppending to the same folder each time.\n\nIf you have enough RAM and cores, though, feel free to run everything at once — go crazy with it!\n\n# Define path to the updated locations Delta table\nlocations_sdf_updated_one &lt;- spark_read_delta(\n  sc,\n  path = file.path(\n    getwd(), \n    \"data\", \n    \"locations_sdf_updated_one\"\n  )\n) |&gt; \n  filter(trip_id &gt;= 40000000 & trip_id &lt;= 40000010) %&gt;% # Filter for only ten rows\n  sdf_register(\"locations_sdf_updated_one_view\")  # Register as a temporary view\n\n\nprint(locations_sdf_updated_one, width=Inf, n=10)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`locations_sdf_updated_one_view`&gt; [?? x 8]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup BoroName  NTA2020\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  \n 1 40000000     40.8     -74.0         1 Manhattan MN0604 \n 2 40000001     40.7     -74.0         1 Manhattan MN0201 \n 3 40000002     40.7     -74.0         1 Manhattan MN0201 \n 4 40000003     40.8     -74.0         1 Manhattan MN0502 \n 5 40000004     40.7     -74.0         1 Manhattan MN0401 \n 6 40000005     40.8     -74.0         1 Manhattan MN0502 \n 7 40000006     40.8     -74.0         1 Manhattan MN0604 \n 8 40000007     40.8     -74.0         1 Manhattan MN0603 \n 9 40000008     40.8     -74.0         1 Manhattan MN0502 \n10 40000009     40.7     -74.0         1 Manhattan MN0603 \n   NTAName                         MdHHIncE\n   &lt;chr&gt;                              &lt;int&gt;\n 1 East Midtown-Turtle Bay           161934\n 2 SoHo-Little Italy-Hudson Square   133847\n 3 SoHo-Little Italy-Hudson Square   133847\n 4 Midtown-Times Square              153871\n 5 Chelsea-Hudson Yards              118915\n 6 Midtown-Times Square              153871\n 7 East Midtown-Turtle Bay           161934\n 8 Murray Hill-Kips Bay              138337\n 9 Midtown-Times Square              153871\n10 Murray Hill-Kips Bay              138337\n# ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part One - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_three.html#loading-worldpop-population-density-dataset",
    "href": "preprocessing_three.html#loading-worldpop-population-density-dataset",
    "title": "4  Part One - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "4.3 Loading WorldPop Population Density dataset",
    "text": "4.3 Loading WorldPop Population Density dataset\nThe difference when using raster data compared to vector data with Apache Sedona is that we do not import raster in its native format directly. Instead, we must first load it as a binary dataframe, and then convert it into its native raster format within Sedona.\nAlso, bear in mind that Sedona only accepts raster files in the following formats:\n- Arc Info ASCII Grid,\n- GeoTIFF, and\n- NetCDF.\nIf your data is in any other raster format, you will first need to convert it to one of these supported formats.\nI have found GDAL to be particularly useful for converting between different raster formats. For this tutorial, I used GDAL (command line) to compress the original US population density raster file, and then clip it using the NYC boundaries shapefile. I used the Deflate lossless compression algorithm. You want to work with as small a file as possible as the processing can be quite memory intensive.\n\n# Compressing the raster file\ngdal_translate -co COMPRESS=DEFLATE usa_pd_2016_1km_UNadj_clipped.tif usa_pd_2016_1km_UNadj_compressed.tif\n\n# Clipping the compressed raster using a shapefile boundary\ngdalwarp -cutline nynta2020.shp -crop_to_cutline usa_pd_2016_1km_UNadj_compressed.tif nyc.tif\n\n\n# Load the raster data for world population (NYC)\nworld_pop_raster_filepath &lt;- file.path(\n  getwd(),\n  \"data\",\n  \"raster\",\n  \"worldpop\",\n  \"nyc.tif\"\n)\n\n\n# Read the raster data as a binary file\nworld_pop_binary &lt;- spark_read_binary(\n  sc,\n  dir = world_pop_raster_filepath,\n  name = \"worldpop\"\n)\n\nWe obtain raster geometry from our GeoTiff data.\n\n# Register the world population raster as a temporary view\nworld_pop_binary |&gt; sdf_register(\"worldpop_view\")\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`worldpop_view`&gt; [?? x 4]\n# Database: spark_connection\n  path                                                modificationTime    length\n  &lt;chr&gt;                                               &lt;dttm&gt;               &lt;int&gt;\n1 file:/Users/rodgersiradukunda/Library/CloudStorage… 2025-02-27 01:46:58  26274\n# ℹ 1 more variable: content &lt;arrw_bnr&gt;\n\n# Extract raster data from the GeoTiff file using Sedona\nworldpop_sdf &lt;- sdf_sql(\n  sc,\n  \"\n  SELECT RS_FromGeoTiff(content) AS raster FROM worldpop_view\n  \"\n)\n\n# Register the raster data as a temporary view\nworldpop_sdf |&gt; sdf_register(\"worldpop_view\") |&gt; compute()\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   table&lt;`dbplyr_BhE89dZkou`&gt; [?? x 1]\n# Database: spark_connection\n# ℹ 1 more variable: raster &lt;arrw_bnr&gt;\n\nworldpop_sdf %&gt;% glimpse()\n\nRows: ??\n\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\nColumns: 1\nDatabase: spark_connection\n$ raster &lt;arrw_bnr&gt; &lt;00, 10, 00, 00, 00, 67, 65, 6f, 74, 69, 66, 66, 5f, 63, 6…\n\n\nWe can retrieve metadata from our raster file, including:\n- The upper left coordinates of the raster (in the raster’s coordinate system units),\n- The width and height of the raster (in number of pixels),\n- The spatial resolution of each pixel (in units of the raster’s CRS),\n- Any skew or rotation of the raster (if present),\n- The SRID (spatial reference system identifier) of the raster’s coordinate system,\n- The number of bands, and\n- Tile width and height.\nIn our case:\n- Upper left X coordinate: -74.25125\n- Upper left Y coordinate: 40.90792 (both in degrees as the CRS is WGS84)\n- Raster size: 66 x 49 pixels (quite small)\n- Pixel resolution: 0.00833 x -0.00833 degrees\n- Skew: 0 in both x and y directions (i.e., no skew)\n- SRID: 4326 (WGS 84)\n- Number of bands: 2\n- Tile width: 66, Tile height: 15\nAll this information is important when interpreting and working with raster data, especially when performing coordinate-based queries.\n\n# Retrieve and view metadata for the world population raster\nworldpop_sdf_metadata &lt;- sdf_sql(\n  sc,\n  \"\n  SELECT RS_MetaData(raster) FROM worldpop_view\n  \"\n) |&gt; collect()\n\nWarning in arrow_enabled_object.spark_jobj(sdf): Arrow disabled due to columns:\nrs_metadata(raster)\n\n\n\noptions(width = 100)\n\n# Glimpse at the metadata information\nworldpop_sdf_metadata |&gt; glimpse()\n\nRows: 1\nColumns: 1\n$ `rs_metadata(raster)` &lt;list&gt; [-74.25125, 40.90792, 66, 49, 0.008333333, -0.008333333, 0, 0, 4326…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part One - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_three.html#joining-point-data-with-raster-data",
    "href": "preprocessing_three.html#joining-point-data-with-raster-data",
    "title": "4  Part One - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "4.4 Joining point data with raster data",
    "text": "4.4 Joining point data with raster data\nWe now conduct the join using Spatial SQL, as it is much easier and more intuitive than using Apache Sedona’s R functions for raster operations in my opinion.\nBy leveraging Spatial SQL, we can directly query raster values at specific pickup and dropoff coordinates, simplifying what would otherwise be a more complex process if done via function-based syntax.\n\n# Perform a spatial join between the locations and the world population data to calculate population density\nlocations_sdf_updated_two &lt;- sdf_sql(\n  sc,\n  \"\n  SELECT \n    /*+ BROADCAST(w) */ l.*, RS_Value(w.raster, ST_Point(l.longitude, l.latitude)) AS pop_density\n  FROM\n    locations_sdf_updated_one_view l\n  LEFT JOIN worldpop_view w\n    ON RS_Intersects(w.raster, ST_POINT(l.longitude, l.latitude))\n  \"\n) \n\nWe can now take a look at the result of our join below.\n\n# Glimpse at the updated data with population density\nlocations_sdf_updated_two %&gt;% glimpse()\n\nRows: ??\n\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer range\n\n\nColumns: 9\nDatabase: spark_connection\n$ trip_id     &lt;dbl&gt; 40000000, 40000001, 40000002, 40000003, 40000004, 40000005, 40000006, 40000007…\n$ latitude    &lt;dbl&gt; 40.75250, 40.72693, 40.72448, 40.76216, 40.74357, 40.75565, 40.75845, 40.75144…\n$ longitude   &lt;dbl&gt; -73.97816, -74.00348, -74.00199, -73.97910, -73.99427, -73.97939, -73.95974, -…\n$ is_pickup   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ BoroName    &lt;chr&gt; \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manhattan\", …\n$ NTA2020     &lt;chr&gt; \"MN0604\", \"MN0201\", \"MN0201\", \"MN0502\", \"MN0401\", \"MN0502\", \"MN0604\", \"MN0603\"…\n$ NTAName     &lt;chr&gt; \"East Midtown-Turtle Bay\", \"SoHo-Little Italy-Hudson Square\", \"SoHo-Little Ita…\n$ MdHHIncE    &lt;int&gt; 161934, 133847, 133847, 153871, 118915, 153871, 161934, 138337, 153871, 138337…\n$ pop_density &lt;dbl&gt; 4927.671, 22022.211, 14979.832, 14245.537, 37489.418, 4927.671, 41234.668, 206…\n\n# Print a preview of the resulting dataframe with specific formatting options\nprint(locations_sdf_updated_two, n=10, width=Inf)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer range\n\n\n# Source:   table&lt;`sparklyr_tmp_7ab5ae36_1fb6_4826_9be0_12adb5f8846b`&gt; [?? x 9]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup BoroName  NTA2020 NTAName                         MdHHIncE\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                              &lt;int&gt;\n 1 40000000     40.8     -74.0         1 Manhattan MN0604  East Midtown-Turtle Bay           161934\n 2 40000001     40.7     -74.0         1 Manhattan MN0201  SoHo-Little Italy-Hudson Square   133847\n 3 40000002     40.7     -74.0         1 Manhattan MN0201  SoHo-Little Italy-Hudson Square   133847\n 4 40000003     40.8     -74.0         1 Manhattan MN0502  Midtown-Times Square              153871\n 5 40000004     40.7     -74.0         1 Manhattan MN0401  Chelsea-Hudson Yards              118915\n 6 40000005     40.8     -74.0         1 Manhattan MN0502  Midtown-Times Square              153871\n 7 40000006     40.8     -74.0         1 Manhattan MN0604  East Midtown-Turtle Bay           161934\n 8 40000007     40.8     -74.0         1 Manhattan MN0603  Murray Hill-Kips Bay              138337\n 9 40000008     40.8     -74.0         1 Manhattan MN0502  Midtown-Times Square              153871\n10 40000009     40.7     -74.0         1 Manhattan MN0603  Murray Hill-Kips Bay              138337\n   pop_density\n         &lt;dbl&gt;\n 1       4928.\n 2      22022.\n 3      14980.\n 4      14246.\n 5      37489.\n 6       4928.\n 7      41235.\n 8      20680.\n 9      14246.\n10      44076.\n# ℹ more rows\n\n\nPerfect! We have successfully obtained approximate population density values for each pickup and dropoff location.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part One - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_three.html#saving-the-data",
    "href": "preprocessing_three.html#saving-the-data",
    "title": "4  Part One - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "4.5 Saving the data",
    "text": "4.5 Saving the data\nWriting the updated data to file for further processing.\n\n# Define file path for saving the updated dataframe\nlocations_sdf_updated_two_file_path &lt;- file.path(\n  getwd(), \n  \"data\", \n  \"locations_sdf_updated_two\"\n)\n\n# Save the final dataframe as a Delta table\nspark_write_delta(\n  locations_sdf_updated_two,\n  path = locations_sdf_updated_two_file_path,\n  mode = \"append\"  # Overwrite any existing data at the location\n)\n\n\n# Disconnect from the Spark session\nspark_disconnect(sc)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part One - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_four.html",
    "href": "preprocessing_four.html",
    "title": "5  Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "",
    "text": "5.1 Introduction\nFor this chapter, we would like to find out the land cover classification associated with specific pickup and dropoff points. Our assumption is that the type of area where one picks up a taxi or gets dropped off may influence how long the trip takes. Again, do not dwell too much on this assumption — the main objective is to demonstrate another way of extracting data from raster files.\nWe shall make use of the Local Climate Zones (LCZ) Map from the World Urban Database and Access Portal Tools (WUDAPT). The US version of this dataset can be accessed here. This dataset contains 17 urban land cover classifications, ranging from compact high-rise buildings to water bodies.\nWe downloaded a version that was already in EPSG:4326 CRS and clipped it based on the NYC boundary. For this chapter, we demonstrate how to divide a raster image into tiles before performing analysis. This is particularly useful when working with large raster files that cannot be processed in their entirety. By dividing the raster into tiles, we make the analysis more manageable and efficient.\nNote: Raster processing is computationally intensive, so it is preferable to work with smaller files, especially when operating in local mode. You will know a raster file is too large when you receive an error upon attempting to view its contents.\nImportant: The Spark configuration used in this chapter is the same as that used in Chapter 2, and so it is not repeated here for brevity.\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\n\n\nAttaching package: 'sparklyr'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n&lt;jobj[23]&gt;\n  org.apache.spark.sql.SparkSession\n  org.apache.spark.sql.SparkSession@439418ab",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_four.html#loading-the-data",
    "href": "preprocessing_four.html#loading-the-data",
    "title": "5  Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "5.2 Loading the data",
    "text": "5.2 Loading the data\nWe start by loading our most updated locations data.\n\n# Read location data stored in Delta format\nlocations_sdf_updated_two &lt;- spark_read_delta(\n  sc,\n  path = file.path(getwd(), \"data\", \"locations_sdf_updated_two\")\n) |&gt; \n  filter(trip_id &gt;= 40000000 & trip_id &lt;= 40000010) %&gt;% # filter for only ten rows \n  sdf_register(\"locations_sdf_updated_two_view\")  # Register as temporary view for SQL\n\nWe then load our raster data.\n\n# Define path to raster file (LCZ map)\nwudapt_raster_filepath &lt;- file.path(\n  getwd(),\n  \"data\",\n  \"raster\",\n  \"wudapt\",\n  \"CONUS_LCZ_map_NLCD_v1.0_cropped.tif\"\n)\n\n\n# Read raster as binary using Spark\nwudapt_binary &lt;- spark_read_binary(\n  sc,\n  dir = wudapt_raster_filepath,\n  name = \"wudapt_binary_view\"\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_four.html#creating-raster-tiles",
    "href": "preprocessing_four.html#creating-raster-tiles",
    "title": "5  Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "5.3 Creating raster tiles",
    "text": "5.3 Creating raster tiles\nHere, we are assuming that our raster data is too large to be processed in its entirety. To address this, we divide it into 256 by 256 tiles. By doing so, we make it more analysis-friendly, as we can first identify a specific tile to work on and then perform analysis on only that tile, instead of the entire raster.\nNote: The clipped dataset used here was actually not large, but we proceeded with this method for demonstration purposes to show how one would handle genuinely large raster datasets.\n\n# Explode raster into tiles (256x256) for spatial operations and cache in memory\nwudapt_raster_tiles &lt;- sdf_sql(\n  sc,\n  \"\n  SELECT RS_TileExplode(RS_FromGeoTiff(content), 256, 256) AS (x, y, tile) FROM wudapt_binary_view\n  \"\n) %&gt;%\n  sdf_register(\"wudapt_raster_tiles_view\")\n\n# Quick look at raster tiles structure\nwudapt_raster_tiles %&gt;% glimpse()\n\nRows: ??\n\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\nColumns: 3\nDatabase: spark_connection\n$ x    &lt;int&gt; 0, 1, 0, 1\n$ y    &lt;int&gt; 0, 0, 1, 1\n$ tile &lt;arrw_bnr&gt; &lt;00, 0f, 00, 00, 00, 67, 65, 6e, 65, 72, 69, 63, 43, 6f, 76, 65, …\n\n\nAs you can see below, the spatial SQL query is divided into two parts:\n\nFirst, we locate the tile that a given coordinate pair (pickup or dropoff point) belongs to.\n\nSecond, we use that specific tile to find the actual land classification value associated with that location.\n\nThis two-step approach makes it efficient to work with large raster datasets by limiting the analysis to only relevant tiles, rather than processing the entire raster at once.\n\n# Spatial join: match location points with corresponding LCZ tiles and assign LCZ labels\nlocations_sdf_updated_three &lt;- sdf_sql(\n  sc,\n  \"\n  WITH matched_tiles AS (\n    SELECT l.*, w.tile\n      FROM wudapt_raster_tiles_view w\n    JOIN locations_sdf_updated_two_view l\n      ON RS_Intersects(w.tile, ST_Point(l.longitude, l.latitude))\n  )\n\n  SELECT *,\n    TRY_CAST(RS_Value(tile, ST_Point(longitude, latitude)) AS INT) AS lcz_class,\n    CASE\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 1 THEN 'Compact highrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 2 THEN 'Compact midrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 3 THEN 'Compact lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 4 THEN 'Open highrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 5 THEN 'Open midrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 6 THEN 'Open lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 7 THEN 'Lightweight lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 8 THEN 'Large lowrise'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 9 THEN 'Sparsely built'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 10 THEN 'Heavy industry'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 11 THEN 'Dense trees'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 12 THEN 'Scattered trees'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 13 THEN 'Bush, scrub'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 14 THEN 'Low plants'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 15 THEN 'Bare rock or paved'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 16 THEN 'Bare soil or sand'\n      WHEN RS_Value(tile, ST_Point(longitude, latitude)) = 17 THEN 'Water'\n      ELSE 'Unknown'\n    END AS lcz_label\n  FROM matched_tiles\n  \"\n)\n\nWe now remove the tile geometry from our dataset because delta cannot serialise geometry columns, and also because we do not need the geometry for our intended analysis. This helps keep the dataset clean and light for storage and further processing.\n\n# Drop RasterUDT (tile) column before saving as Delta (unsupported type in Delta)\nlocations_sdf_updated_three &lt;- locations_sdf_updated_three %&gt;%\n  select(-tile)\n\nThe updated data now looks as shown below. As you can see, we went from only having coordinates to obtaining richer information about our locations by leveraging additional geospatial datasets.\nNot too bad for using a simple laptop to process tens of millions of rows!\n\n# Preview updated data\nlocations_sdf_updated_three %&gt;% glimpse()\n\nRows: ??\n\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\nColumns: 11\nDatabase: spark_connection\n$ trip_id     &lt;dbl&gt; 40000002, 40000009, 40000008, 40000005, 40000010, 40000003…\n$ latitude    &lt;dbl&gt; 40.72448, 40.77977, 40.76196, 40.75565, 40.73432, 40.74904…\n$ longitude   &lt;dbl&gt; -74.00199, -73.98648, -73.97881, -73.97939, -73.99419, -73…\n$ is_pickup   &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1…\n$ BoroName    &lt;chr&gt; \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manhattan\", \"Manha…\n$ NTA2020     &lt;chr&gt; \"MN0201\", \"MN0701\", \"MN0502\", \"MN0502\", \"MN0202\", \"MN0501\"…\n$ NTAName     &lt;chr&gt; \"SoHo-Little Italy-Hudson Square\", \"Upper West Side-Lincol…\n$ MdHHIncE    &lt;int&gt; 133847, 158165, 153871, 153871, 175436, 167458, 138337, 11…\n$ pop_density &lt;dbl&gt; 14979.832, 23416.408, 14245.537, 4927.671, 37463.500, 1587…\n$ lcz_class   &lt;int&gt; 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1…\n$ lcz_label   &lt;chr&gt; \"Compact midrise\", \"Compact highrise\", \"Compact highrise\",…\n\n\n\n# Print formatted preview for review\nwithr::with_options(\n  list(pillar.sigfig = 6),\n  print(locations_sdf_updated_three, n = 10, width = Inf)\n)\n\nWarning in arrow_collect(object, ...): NAs introduced by coercion to integer\nrange\n\n\n# Source:   SQL [?? x 11]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup BoroName  NTA2020\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  \n 1 40000002  40.7245  -74.0020         1 Manhattan MN0201 \n 2 40000009  40.7798  -73.9865         0 Manhattan MN0701 \n 3 40000008  40.7620  -73.9788         1 Manhattan MN0502 \n 4 40000005  40.7557  -73.9794         1 Manhattan MN0502 \n 5 40000010  40.7343  -73.9942         0 Manhattan MN0202 \n 6 40000003  40.7490  -73.9903         0 Manhattan MN0501 \n 7 40000005  40.7419  -73.9746         0 Manhattan MN0603 \n 8 40000004  40.7436  -73.9943         1 Manhattan MN0401 \n 9 40000007  40.7514  -73.9737         1 Manhattan MN0603 \n10 40000002  40.7270  -73.9939         0 Manhattan MN0202 \n   NTAName                             MdHHIncE pop_density lcz_class\n   &lt;chr&gt;                                  &lt;int&gt;       &lt;dbl&gt;     &lt;int&gt;\n 1 SoHo-Little Italy-Hudson Square       133847    14979.8          2\n 2 Upper West Side-Lincoln Square        158165    23416.4          1\n 3 Midtown-Times Square                  153871    14245.5          1\n 4 Midtown-Times Square                  153871     4927.67         1\n 5 Greenwich Village                     175436    37463.5          2\n 6 Midtown South-Flatiron-Union Square   167458    15877.1          1\n 7 Murray Hill-Kips Bay                  138337    29922.4          1\n 8 Chelsea-Hudson Yards                  118915    37489.4          2\n 9 Murray Hill-Kips Bay                  138337    20680.5          1\n10 Greenwich Village                     175436    31733.8          2\n   lcz_label       \n   &lt;chr&gt;           \n 1 Compact midrise \n 2 Compact highrise\n 3 Compact highrise\n 4 Compact highrise\n 5 Compact midrise \n 6 Compact highrise\n 7 Compact highrise\n 8 Compact midrise \n 9 Compact highrise\n10 Compact midrise \n# ℹ more rows\n\n\nFinally, we save the data for further processing.\n\n# Define output path for writing final dataset\nlocations_sdf_updated_three_file_path &lt;- file.path(\n  getwd(),\n  \"data\",\n  \"locations_sdf_updated_three\"\n)\n\n# Write enriched data back to Delta format (append mode)\nspark_write_delta(\n  locations_sdf_updated_three,\n  path = locations_sdf_updated_three_file_path,\n  mode = \"append\"\n)\n\nAnd disconnect from our spark instance.\n\n# Disconnect Spark session to free resources\nspark_disconnect(sc)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Part Two - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  },
  {
    "objectID": "preprocessing_five.html",
    "href": "preprocessing_five.html",
    "title": "6  Combining Updated Locations Data with Initial Trip Data",
    "section": "",
    "text": "6.1 Introduction\nIf you recall, we had initially separated location and non-location data so as to gather more location-related data, such as median household income and population density around pickup and dropoff points.\nHaving accomplished that, we will now combine the updated locations data with the rest of the trip data.\nThis will be the last chapter for me, but please go further — create maps based on this data, and perform machine learning analysis too. You can find Sparklyr machine learning documentation here.\nIt has been a joy to come this far, and I hope you have learnt something new throughout this whole tutorial.\nFYI, because we are not using Apache Sedona in this part, we are using the same Spark configuration as that used in Chapter One.\nAnyhow, let us get to work!\n# Load required libraries\nlibrary(sparklyr)   # Spark connection and data manipulation\n\n\nAttaching package: 'sparklyr'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(dplyr)      # Data manipulation functions\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Install and set up Spark environment\nspark_install(\"3.5.5\")  # Install the specific version of Spark (3.5.5)\n\n# Set Java and Spark home directory paths\nSys.setenv(\"JAVA_HOME\"=\"/Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home\")  # Set Java home directory for Spark\nSys.setenv(\"SPARK_HOME\"=spark_home_dir(version = \"3.5.5\"))  # Set Spark home directory\n\n# Define working directory path for file management\nworking_dir &lt;- \"/Users/rodgersiradukunda/Library/CloudStorage/OneDrive-TheUniversityofLiverpool/geospatial_docker\"\nsetwd(working_dir)  # Set the working directory\n\n# Define path for Spark data\nspark_dir &lt;- file.path(getwd(), \"data\", \"spark\")\n\n# Create an empty list for Spark configuration settings\nconfig &lt;- list()\n\n# Set Spark configurations for memory and performance optimisation\nconfig$`sparklyr.shell.driver-java-options` &lt;- paste0(\"-Djava.io.tmpdir=\", spark_dir)  # Set temporary directory for Spark\nconfig$`sparklyr.shell.driver-java-options` &lt;- \"-XX:+UseCompressedOops\"  # Use compressed Oops for JVM performance\nconfig$`sparklyr.shell.driver-memory` &lt;- '10G'  # Allocate 8GB of memory for the Spark driver\nconfig$spark.memory.fraction &lt;- 0.7  # Set fraction of heap memory used for Spark storage\nconfig$spark.sql.shuffle.partitions.local &lt;- 24  # Set shuffle partitions (local setting based on workload)\nconfig$spark.driver.extraJavaOptions &lt;- \"-Xmx8G\"  # Set extra memory for driver\nconfig$spark.serializer &lt;- \"org.apache.spark.serializer.KryoSerializer\"  # Use KryoSerializer for better performance\nconfig$spark.memory.offHeap.enabled &lt;- \"true\"  # Enable off-heap memory usage\nconfig$spark.memory.offHeap.size &lt;- \"4g\"  # Set 4GB for off-heap memory\nconfig$spark.sql.shuffle.spill &lt;- \"false\"  # Disable shuffle spill to disk\nconfig$spark.cleaner.periodicGC.interval &lt;- \"60s\"  # Periodic garbage collection interval\nconfig$spark.sql.files.maxPartitionBytes &lt;- \"200m\"  # Set max partition size for shuffle files\nconfig$spark.sql.adaptive.enabled &lt;- \"true\"  # Enable adaptive query execution\n\n# Connect to Spark with the specified configurations\nsc &lt;- spark_connect(\n  master = \"local[*]\",  # Use all available cores for local execution\n  config = config,      # Use the specified configurations\n  packages = \"delta\"    # Install the Delta Lake package for optimised storage\n)\n\n# Open Spark web UI for monitoring the connection\nspark_web(sc)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining Updated Locations Data with Initial Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_five.html#load-the-datasets",
    "href": "preprocessing_five.html#load-the-datasets",
    "title": "6  Combining Updated Locations Data with Initial Trip Data",
    "section": "6.2 Load the datasets",
    "text": "6.2 Load the datasets\nWe start by loading the datasets.\nAs you can see below, in addition to partitioning the data into 24 parts, we also specify the column to partition by, and use trip_id which is common to both datasets.\nThis is because we shall later join the datasets based on this column, and we want rows with the same trip_id to be in the same partitions so as to minimise shuffling of data, which is quite computationally intensive.\n\n# Read locations dataset in Delta format and register as a SQL view for querying\nlocations_sdf_updated_three &lt;- spark_read_delta(\n  sc,\n  path = file.path(getwd(), \"data\", \"locations_sdf_updated_three\")\n) |&gt; \n  filter(trip_id &gt;= 40000000 & trip_id &lt;= 40000010) %&gt;% # filter for only ten rows \n  sdf_register(\"locations_sdf_updated_three_view\")  # Register as view for SQL queries\n\n# Read trip data in Delta format and register as a SQL view\ntrip_data_sdf &lt;- spark_read_delta(\n  sc,\n  path = file.path(getwd(), \"data\", \"trip_data_sdf\")\n) %&gt;% \n  filter(trip_id &gt; 40000000) %&gt;%  # Optional filtering\n  sdf_repartition(partitions = 24, partition_by = \"trip_id\") %&gt;%  # Repartition for optimal join performance\n  sdf_register(\"trip_data_sdf\")\n\nJust a refresher on how these datasets look.\n\nprint(locations_sdf_updated_three, width=Inf)\n\n# Source:   table&lt;`locations_sdf_updated_three_view`&gt; [?? x 11]\n# Database: spark_connection\n    trip_id latitude longitude is_pickup BoroName  NTA2020\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  \n 1 40000009     40.7     -74.0         1 Manhattan MN0603 \n 2 40000006     40.8     -74.0         1 Manhattan MN0604 \n 3 40000007     40.7     -74.0         0 Manhattan MN0501 \n 4 40000005     40.8     -74.0         1 Manhattan MN0502 \n 5 40000006     40.8     -74.0         0 Manhattan MN0801 \n 6 40000004     40.7     -74.0         1 Manhattan MN0401 \n 7 40000002     40.7     -74.0         0 Manhattan MN0202 \n 8 40000004     40.7     -74.0         0 Manhattan MN0501 \n 9 40000003     40.7     -74.0         0 Manhattan MN0501 \n10 40000007     40.8     -74.0         1 Manhattan MN0603 \n   NTAName                                     MdHHIncE pop_density lcz_class\n   &lt;chr&gt;                                          &lt;int&gt;       &lt;dbl&gt;     &lt;int&gt;\n 1 Murray Hill-Kips Bay                          138337      44076.         1\n 2 East Midtown-Turtle Bay                       161934      41235.         1\n 3 Midtown South-Flatiron-Union Square           167458      29773.         1\n 4 Midtown-Times Square                          153871       4928.         1\n 5 Upper East Side-Lenox Hill-Roosevelt Island   133349      21597.         1\n 6 Chelsea-Hudson Yards                          118915      37489.         2\n 7 Greenwich Village                             175436      31734.         2\n 8 Midtown South-Flatiron-Union Square           167458      29773.         2\n 9 Midtown South-Flatiron-Union Square           167458      15877.         1\n10 Murray Hill-Kips Bay                          138337      20680.         1\n   lcz_label       \n   &lt;chr&gt;           \n 1 Compact highrise\n 2 Compact highrise\n 3 Compact highrise\n 4 Compact highrise\n 5 Compact highrise\n 6 Compact midrise \n 7 Compact midrise \n 8 Compact midrise \n 9 Compact highrise\n10 Compact highrise\n# ℹ more rows\n\n\n\nprint(trip_data_sdf, width=Inf, n=10)\n\n# Source:   table&lt;`trip_data_sdf`&gt; [?? x 30]\n# Database: spark_connection\n   VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n      &lt;int&gt; &lt;dttm&gt;               &lt;dttm&gt;                          &lt;int&gt;\n 1        1 2016-01-25 11:17:40  2016-01-25 11:25:12                 1\n 2        1 2016-01-25 11:22:58  2016-01-25 11:27:02                 1\n 3        1 2016-01-25 11:26:04  2016-01-25 11:36:49                 1\n 4        1 2016-01-25 11:23:03  2016-01-25 11:43:07                 1\n 5        1 2016-01-25 11:26:02  2016-01-25 11:43:07                 1\n 6        1 2016-01-25 11:29:33  2016-01-25 11:47:02                 1\n 7        1 2016-01-25 11:38:11  2016-01-25 11:50:29                 1\n 8        2 2016-01-25 11:34:26  2016-01-25 11:55:53                 1\n 9        2 2016-01-25 11:48:14  2016-01-25 11:56:11                 3\n10        2 2016-01-25 11:37:59  2016-01-25 11:57:48                 1\n   trip_distance RateCodeID store_and_fwd_flag payment_type fare_amount extra\n           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;              &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1          0.6  1          N                  1                    6.5     0\n 2          0.8  1          N                  2                    5       0\n 3          1.2  1          N                  1                    8.5     0\n 4          2.3  1          N                  1                   13.5     0\n 5          1.9  1          N                  2                   12       0\n 6          1.7  1          N                  2                   12       0\n 7          1.5  1          N                  1                    9.5     0\n 8          4.04 1          N                  1                   18       0\n 9          1.19 1          N                  1                    7       0\n10          1.36 1          N                  2                   13       0\n   mta_tax tip_amount tolls_amount improvement_surcharge total_amount\n     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n 1     0.5       1.45            0                   0.3         8.75\n 2     0.5       0               0                   0.3         5.8 \n 3     0.5       1.85            0                   0.3        11.2 \n 4     0.5       2.86            0                   0.3        17.2 \n 5     0.5       0               0                   0.3        12.8 \n 6     0.5       0               0                   0.3        12.8 \n 7     0.5       1.5             0                   0.3        11.8 \n 8     0.5       3.76            0                   0.3        22.6 \n 9     0.5       1.56            0                   0.3         9.36\n10     0.5       0               0                   0.3        13.8 \n   improvement_surcharge_imputed pickup_hour pickup_dayofweek pickup_week\n                           &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;\n 1                           0.3          11 Mon                        4\n 2                           0.3          11 Mon                        4\n 3                           0.3          11 Mon                        4\n 4                           0.3          11 Mon                        4\n 5                           0.3          11 Mon                        4\n 6                           0.3          11 Mon                        4\n 7                           0.3          11 Mon                        4\n 8                           0.3          11 Mon                        4\n 9                           0.3          11 Mon                        4\n10                           0.3          11 Mon                        4\n   pickup_month dropoff_hour dropoff_dayofweek dropoff_week dropoff_month\n          &lt;int&gt;        &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt;\n 1            1           11 Mon                          4             1\n 2            1           11 Mon                          4             1\n 3            1           11 Mon                          4             1\n 4            1           11 Mon                          4             1\n 5            1           11 Mon                          4             1\n 6            1           11 Mon                          4             1\n 7            1           11 Mon                          4             1\n 8            1           11 Mon                          4             1\n 9            1           11 Mon                          4             1\n10            1           11 Mon                          4             1\n   is_weekend_pickup is_weekend_dropoff is_rush_hour_pickup trip_distance_scaled\n               &lt;dbl&gt;              &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n 1                 0                  0                   0               -0.648\n 2                 0                  0                   0               -0.591\n 3                 0                  0                   0               -0.478\n 4                 0                  0                   0               -0.166\n 5                 0                  0                   0               -0.280\n 6                 0                  0                   0               -0.336\n 7                 0                  0                   0               -0.393\n 8                 0                  0                   0                0.327\n 9                 0                  0                   0               -0.481\n10                 0                  0                   0               -0.433\n   fare_amount_scaled  trip_id\n                &lt;dbl&gt;    &lt;dbl&gt;\n 1            -0.567  40000002\n 2            -0.713  40000022\n 3            -0.373  40000097\n 4             0.114  40000128\n 5            -0.0319 40000129\n 6            -0.0319 40000167\n 7            -0.275  40000198\n 8             0.552  40000238\n 9            -0.519  40000240\n10             0.0655 40000260\n# ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining Updated Locations Data with Initial Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_five.html#joining-the-datasets",
    "href": "preprocessing_five.html#joining-the-datasets",
    "title": "6  Combining Updated Locations Data with Initial Trip Data",
    "section": "6.3 Joining the datasets",
    "text": "6.3 Joining the datasets\nWe start by joining the non-location data with the pickup location data, renaming certain variables, and dropping others in the process.\n\n# Join trip data with pickup locations using trip_id and rename selected columns for clarity\nmerged_one &lt;- left_join(\n  trip_data_sdf,\n  locations_sdf_updated_three %&gt;% filter(is_pickup == 1),\n  by = \"trip_id\"\n) %&gt;% \n  rename(\n    pickup_borough = BoroName,\n    pickup_neighbourhood = NTAName, \n    pickup_neigh_hhincome = MdHHIncE,\n    pickup_pop_density = pop_density,\n    pickup_lcz_label = lcz_label\n  ) %&gt;% \n  select(\n    trip_id,\n    VendorID, \n    tpep_pickup_datetime, \n    tpep_dropoff_datetime, \n    passenger_count,\n    trip_distance,\n    pickup_hour,\n    pickup_dayofweek,\n    pickup_week,\n    pickup_month,\n    dropoff_hour,\n    dropoff_dayofweek,\n    dropoff_week,\n    dropoff_month,\n    is_weekend_pickup,\n    is_weekend_dropoff,\n    is_rush_hour_pickup,\n    trip_distance_scaled,\n    pickup_borough,\n    pickup_neighbourhood,\n    pickup_neigh_hhincome,\n    pickup_pop_density,\n    pickup_lcz_label\n  )\n\n# Display the merged pickup-enriched dataset\nprint(merged_one, width = Inf, n=10)\n\n# Source:   SQL [?? x 23]\n# Database: spark_connection\n    trip_id VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n      &lt;dbl&gt;    &lt;int&gt; &lt;dttm&gt;               &lt;dttm&gt;                          &lt;int&gt;\n 1 40000002        1 2016-01-25 11:17:40  2016-01-25 11:25:12                 1\n 2 40000022        1 2016-01-25 11:22:58  2016-01-25 11:27:02                 1\n 3 40000097        1 2016-01-25 11:26:04  2016-01-25 11:36:49                 1\n 4 40000128        1 2016-01-25 11:23:03  2016-01-25 11:43:07                 1\n 5 40000129        1 2016-01-25 11:26:02  2016-01-25 11:43:07                 1\n 6 40000167        1 2016-01-25 11:29:33  2016-01-25 11:47:02                 1\n 7 40000198        1 2016-01-25 11:38:11  2016-01-25 11:50:29                 1\n 8 40000238        2 2016-01-25 11:34:26  2016-01-25 11:55:53                 1\n 9 40000240        2 2016-01-25 11:48:14  2016-01-25 11:56:11                 3\n10 40000260        2 2016-01-25 11:37:59  2016-01-25 11:57:48                 1\n   trip_distance pickup_hour pickup_dayofweek pickup_week pickup_month\n           &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;\n 1          0.6           11 Mon                        4            1\n 2          0.8           11 Mon                        4            1\n 3          1.2           11 Mon                        4            1\n 4          2.3           11 Mon                        4            1\n 5          1.9           11 Mon                        4            1\n 6          1.7           11 Mon                        4            1\n 7          1.5           11 Mon                        4            1\n 8          4.04          11 Mon                        4            1\n 9          1.19          11 Mon                        4            1\n10          1.36          11 Mon                        4            1\n   dropoff_hour dropoff_dayofweek dropoff_week dropoff_month is_weekend_pickup\n          &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt;             &lt;dbl&gt;\n 1           11 Mon                          4             1                 0\n 2           11 Mon                          4             1                 0\n 3           11 Mon                          4             1                 0\n 4           11 Mon                          4             1                 0\n 5           11 Mon                          4             1                 0\n 6           11 Mon                          4             1                 0\n 7           11 Mon                          4             1                 0\n 8           11 Mon                          4             1                 0\n 9           11 Mon                          4             1                 0\n10           11 Mon                          4             1                 0\n   is_weekend_dropoff is_rush_hour_pickup trip_distance_scaled pickup_borough\n                &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;         \n 1                  0                   0               -0.648 Manhattan     \n 2                  0                   0               -0.591 &lt;NA&gt;          \n 3                  0                   0               -0.478 &lt;NA&gt;          \n 4                  0                   0               -0.166 &lt;NA&gt;          \n 5                  0                   0               -0.280 &lt;NA&gt;          \n 6                  0                   0               -0.336 &lt;NA&gt;          \n 7                  0                   0               -0.393 &lt;NA&gt;          \n 8                  0                   0                0.327 &lt;NA&gt;          \n 9                  0                   0               -0.481 &lt;NA&gt;          \n10                  0                   0               -0.433 &lt;NA&gt;          \n   pickup_neighbourhood            pickup_neigh_hhincome pickup_pop_density\n   &lt;chr&gt;                                           &lt;int&gt;              &lt;dbl&gt;\n 1 SoHo-Little Italy-Hudson Square                133847             14980.\n 2 &lt;NA&gt;                                               NA                NA \n 3 &lt;NA&gt;                                               NA                NA \n 4 &lt;NA&gt;                                               NA                NA \n 5 &lt;NA&gt;                                               NA                NA \n 6 &lt;NA&gt;                                               NA                NA \n 7 &lt;NA&gt;                                               NA                NA \n 8 &lt;NA&gt;                                               NA                NA \n 9 &lt;NA&gt;                                               NA                NA \n10 &lt;NA&gt;                                               NA                NA \n   pickup_lcz_label\n   &lt;chr&gt;           \n 1 Compact midrise \n 2 &lt;NA&gt;            \n 3 &lt;NA&gt;            \n 4 &lt;NA&gt;            \n 5 &lt;NA&gt;            \n 6 &lt;NA&gt;            \n 7 &lt;NA&gt;            \n 8 &lt;NA&gt;            \n 9 &lt;NA&gt;            \n10 &lt;NA&gt;            \n# ℹ more rows\n\n\nWe now join our updated data with dropoff location data, again renaming certain variables and dropping others.\n\n# Join the merged pickup dataset with dropoff locations and rename/drop unnecessary columns\nmerged_two &lt;- left_join(\n  merged_one,\n  locations_sdf_updated_three %&gt;% filter(is_pickup == 0),\n  by = \"trip_id\"\n) %&gt;% \n  rename(\n    dropoff_borough = BoroName,\n    dropoff_neighbourhood = NTAName, \n    dropoff_neigh_hhincome = MdHHIncE,\n    dropoff_pop_density = pop_density,\n    dropoff_lcz_label = lcz_label\n  ) %&gt;% \n  select(-c(latitude, longitude, is_pickup, lcz_class))  # Drop redundant columns\n\nThis is what our final dataset looks like.\n\n# Print the final merged dataset enriched with both pickup and dropoff spatial context\nprint(merged_two, width = Inf, n=10)\n\n# Source:   SQL [?? x 29]\n# Database: spark_connection\n    trip_id VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n      &lt;dbl&gt;    &lt;int&gt; &lt;dttm&gt;               &lt;dttm&gt;                          &lt;int&gt;\n 1 40000002        1 2016-01-25 11:17:40  2016-01-25 11:25:12                 1\n 2 40000022        1 2016-01-25 11:22:58  2016-01-25 11:27:02                 1\n 3 40000097        1 2016-01-25 11:26:04  2016-01-25 11:36:49                 1\n 4 40000128        1 2016-01-25 11:23:03  2016-01-25 11:43:07                 1\n 5 40000129        1 2016-01-25 11:26:02  2016-01-25 11:43:07                 1\n 6 40000167        1 2016-01-25 11:29:33  2016-01-25 11:47:02                 1\n 7 40000198        1 2016-01-25 11:38:11  2016-01-25 11:50:29                 1\n 8 40000238        2 2016-01-25 11:34:26  2016-01-25 11:55:53                 1\n 9 40000240        2 2016-01-25 11:48:14  2016-01-25 11:56:11                 3\n10 40000260        2 2016-01-25 11:37:59  2016-01-25 11:57:48                 1\n   trip_distance pickup_hour pickup_dayofweek pickup_week pickup_month\n           &lt;dbl&gt;       &lt;int&gt; &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;\n 1          0.6           11 Mon                        4            1\n 2          0.8           11 Mon                        4            1\n 3          1.2           11 Mon                        4            1\n 4          2.3           11 Mon                        4            1\n 5          1.9           11 Mon                        4            1\n 6          1.7           11 Mon                        4            1\n 7          1.5           11 Mon                        4            1\n 8          4.04          11 Mon                        4            1\n 9          1.19          11 Mon                        4            1\n10          1.36          11 Mon                        4            1\n   dropoff_hour dropoff_dayofweek dropoff_week dropoff_month is_weekend_pickup\n          &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt;             &lt;dbl&gt;\n 1           11 Mon                          4             1                 0\n 2           11 Mon                          4             1                 0\n 3           11 Mon                          4             1                 0\n 4           11 Mon                          4             1                 0\n 5           11 Mon                          4             1                 0\n 6           11 Mon                          4             1                 0\n 7           11 Mon                          4             1                 0\n 8           11 Mon                          4             1                 0\n 9           11 Mon                          4             1                 0\n10           11 Mon                          4             1                 0\n   is_weekend_dropoff is_rush_hour_pickup trip_distance_scaled pickup_borough\n                &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;         \n 1                  0                   0               -0.648 Manhattan     \n 2                  0                   0               -0.591 &lt;NA&gt;          \n 3                  0                   0               -0.478 &lt;NA&gt;          \n 4                  0                   0               -0.166 &lt;NA&gt;          \n 5                  0                   0               -0.280 &lt;NA&gt;          \n 6                  0                   0               -0.336 &lt;NA&gt;          \n 7                  0                   0               -0.393 &lt;NA&gt;          \n 8                  0                   0                0.327 &lt;NA&gt;          \n 9                  0                   0               -0.481 &lt;NA&gt;          \n10                  0                   0               -0.433 &lt;NA&gt;          \n   pickup_neighbourhood            pickup_neigh_hhincome pickup_pop_density\n   &lt;chr&gt;                                           &lt;int&gt;              &lt;dbl&gt;\n 1 SoHo-Little Italy-Hudson Square                133847             14980.\n 2 &lt;NA&gt;                                               NA                NA \n 3 &lt;NA&gt;                                               NA                NA \n 4 &lt;NA&gt;                                               NA                NA \n 5 &lt;NA&gt;                                               NA                NA \n 6 &lt;NA&gt;                                               NA                NA \n 7 &lt;NA&gt;                                               NA                NA \n 8 &lt;NA&gt;                                               NA                NA \n 9 &lt;NA&gt;                                               NA                NA \n10 &lt;NA&gt;                                               NA                NA \n   pickup_lcz_label dropoff_borough NTA2020 dropoff_neighbourhood\n   &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;                \n 1 Compact midrise  Manhattan       MN0202  Greenwich Village    \n 2 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 3 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 4 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 5 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 6 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 7 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 8 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n 9 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n10 &lt;NA&gt;             &lt;NA&gt;            &lt;NA&gt;    &lt;NA&gt;                 \n   dropoff_neigh_hhincome dropoff_pop_density dropoff_lcz_label\n                    &lt;int&gt;               &lt;dbl&gt; &lt;chr&gt;            \n 1                 175436              31734. Compact midrise  \n 2                     NA                 NA  &lt;NA&gt;             \n 3                     NA                 NA  &lt;NA&gt;             \n 4                     NA                 NA  &lt;NA&gt;             \n 5                     NA                 NA  &lt;NA&gt;             \n 6                     NA                 NA  &lt;NA&gt;             \n 7                     NA                 NA  &lt;NA&gt;             \n 8                     NA                 NA  &lt;NA&gt;             \n 9                     NA                 NA  &lt;NA&gt;             \n10                     NA                 NA  &lt;NA&gt;             \n# ℹ more rows\n\n\nFinally, we write the data to disk. As stated earlier, feel free to go beyond this tutorial, creating maps based on NYC neighbourhoods and building models. Good luck, and thank you!\n\n# Define output path to save final merged dataset in Delta format\nlocations_sdf_updated_four_file_path &lt;- file.path(\n  getwd(),\n  \"data\",\n  \"locations_sdf_updated_four\"\n)\n\n# Write final dataset in Delta format with append mode to allow incremental writing\nspark_write_delta(\n  merged_two,\n  path = locations_sdf_updated_four_file_path,\n  mode = \"append\"\n)\n\n\n# Disconnect from the Spark session to free resources\nspark_disconnect(sc)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining Updated Locations Data with Initial Trip Data</span>"
    ]
  },
  {
    "objectID": "preprocessing_three.html#references",
    "href": "preprocessing_three.html#references",
    "title": "4  Part One - Processing Raster Data with Apache Sedona and Sparklyr in R",
    "section": "4.6 References",
    "text": "4.6 References\n\nWorldPop (www.worldpop.org - School of Geography and Environmental Science, University of Southampton; Department of Geography and Geosciences, University of Louisville; Departement de Geographie, Universite de Namur) and Center for International Earth Science Information Network (CIESIN), Columbia University (2018). Global High Resolution Population Denominators Project - Funded by The Bill and Melinda Gates Foundation (OPP1134076). https://dx.doi.org/10.5258/SOTON/WP00675",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part One - Processing Raster Data with Apache Sedona and Sparklyr in R</span>"
    ]
  }
]